{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0880ab1a",
   "metadata": {
    "papermill": {
     "duration": 0.011568,
     "end_time": "2023-06-05T22:50:31.883155",
     "exception": false,
     "start_time": "2023-06-05T22:50:31.871587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this workbook, we will train a linear classifier model to be able to label movie reviews as either generally positive or negative. This task, called sentiment analysis, is a very frequently applied tool used by companies to understand the general feeling toward a certain product through automated text analysis. Instead of having to read reviews one by one, we can get a rough snapshot of public opinion very quickly.\n",
    "\n",
    "To train our model, we will use the Sentiment Polarity Data Set v2.0 from [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/) by Pang, Lee and Vaithyanathan. This csv file contains 1000 positive and 1000 negative movie reviews, and each have been labeled as such. By supplying this labeled data to our machine learning algorithm, we can train a model to be able to predict whether any movie review is negative or positive.\n",
    "\n",
    "This Jupyter Notebook will walk you through the code that explores the data, pre-processes the reviews into a format that the machine learning algorithm can understand, and trains and tests the model. We will also demonstrate how the model can predict the sentiment of movie reviews that you write!\n",
    "\n",
    "# Machine Learning Workflow\n",
    "\n",
    "## Loading the data into our workbook\n",
    "\n",
    "First we will load in our data. We will use the popular data manipulation and analysis package called \"pandas\" to read in our csv files and explore the data. The data is already separated into a training set and a testing set on the Kaggle website where we are downloading the data from, so we will load each of these separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4948df14",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:31.906632Z",
     "iopub.status.busy": "2023-06-05T22:50:31.906252Z",
     "iopub.status.idle": "2023-06-05T22:50:32.092390Z",
     "shell.execute_reply": "2023-06-05T22:50:32.091209Z"
    },
    "papermill": {
     "duration": 0.20124,
     "end_time": "2023-06-05T22:50:32.095110",
     "exception": false,
     "start_time": "2023-06-05T22:50:31.893870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews_training = pd.read_csv(\"/kaggle/input/movie-reviews-sentiment-polarity/movie_reviews_train.csv\")\n",
    "reviews_test = pd.read_csv(\"/kaggle/input/movie-reviews-sentiment-polarity/movie_reviews_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b2f16",
   "metadata": {
    "papermill": {
     "duration": 0.010727,
     "end_time": "2023-06-05T22:50:32.116517",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.105790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that our data is loaded, we should take some time to explore it and become familiar with it.\n",
    "\n",
    "## Exploring the data\n",
    "\n",
    "Let's explore our data and see how it's formatted. We will use the `.head()` method from pandas to look at the first 5 rows of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae24181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.140380Z",
     "iopub.status.busy": "2023-06-05T22:50:32.139941Z",
     "iopub.status.idle": "2023-06-05T22:50:32.170202Z",
     "shell.execute_reply": "2023-06-05T22:50:32.169204Z"
    },
    "papermill": {
     "duration": 0.044988,
     "end_time": "2023-06-05T22:50:32.172470",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.127482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>every once in a while you see a film that is s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the love for family is one of the strongest dr...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>after the terminally bleak reservoir dogs and ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>( warning to those who have not seen seven : ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>having not seen , \" who framed roger rabbit \" ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content Label\n",
       "0  every once in a while you see a film that is s...   pos\n",
       "1  the love for family is one of the strongest dr...   pos\n",
       "2  after the terminally bleak reservoir dogs and ...   pos\n",
       "3   ( warning to those who have not seen seven : ...   pos\n",
       "4  having not seen , \" who framed roger rabbit \" ...   pos"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03390aa2",
   "metadata": {
    "papermill": {
     "duration": 0.010889,
     "end_time": "2023-06-05T22:50:32.194368",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.183479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see the index column on the left, and two labeled columns in our dataset. We have the \"Content\" column, which contains the movie reviews. The other column \"Label\" contains \"pos\", which I am assuming means it has been labeled as a positive review. Let's check out a random row and read the movie review to see if we agree with the existing labels. The pandas `.loc` method lets us pick out certain values by row index label and column name. I randomly chose row 42, and printed out both the 'Content' and 'Label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8ce2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.218840Z",
     "iopub.status.busy": "2023-06-05T22:50:32.217752Z",
     "iopub.status.idle": "2023-06-05T22:50:32.223874Z",
     "shell.execute_reply": "2023-06-05T22:50:32.223034Z"
    },
    "papermill": {
     "duration": 0.021224,
     "end_time": "2023-06-05T22:50:32.226494",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.205270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sometimes a movie comes along that falls somewhat askew of the rest . \n",
      "some people call it \" original \" or \" artsy \" or \" abstract \" . \n",
      "some people simply call it \" trash \" . \n",
      "a life less ordinary is sure to bring about mixed feelings . \n",
      "definitely a generation-x aimed movie , a life less ordinary has everything from claymation to profane angels to a karaoke-based musical dream sequence . \n",
      "whew ! \n",
      "anyone in their 30's or above is probably not going to grasp what can be enjoyed about this film . \n",
      "it's somewhat silly , it's somewhat outrageous , and it's definitely not your typical romance story , but for the right audience , it works . \n",
      "a lot of hype has been surrounding this film due to the fact that it comes to us from the same team that brought us trainspotting . \n",
      "well sorry folks , but i haven't seen trainspotting so i can't really compare . \n",
      "whether that works in this film's favor or not is beyond me . \n",
      "but i do know this : ewan mcgregor , whom i had never had the pleasure of watching , definitely charmed me . \n",
      "he was great ! \n",
      "cameron diaz's character was uneven and a bit hard to grasp . \n",
      "the audience may find it difficult to care about her , thus discouraging the hopes of seeing her unite with mcgregor after we are immediately sucked into caring about and identifying with him . \n",
      "misguided ? \n",
      "you bet . \n",
      "loveable ? \n",
      "you bet . \n",
      "a life less ordinary was a delight and even had a bonus for me when i realized it was filmed in my hometown of salt lake city , utah . \n",
      "this was just one more thing i didn't know about this movie when i sat down with a five dollar order of nachos and a three dollar coke . \n",
      "maybe not knowing the premise behind this film made for a pleasant surprise , but i think even if i had known , i would have been just as happy . \n",
      "a life less ordinary is quirky , eccentric , and downright charming ! \n",
      "not for everyone , but a definite change of pace for your typical night at the movies . \n",
      "\n",
      "The sentiment has been labeled as pos\n"
     ]
    }
   ],
   "source": [
    "print(reviews_training.loc[42,'Content'])\n",
    "print(\"The sentiment has been labeled as\", reviews_training.loc[42,'Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcca768",
   "metadata": {
    "papermill": {
     "duration": 0.010949,
     "end_time": "2023-06-05T22:50:32.248588",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.237639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I would agree with the label! Now that we've looked at some examples, let's make sure the testing set we loaded earlier is in a similar format. We will use the `.head()` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98412146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.273504Z",
     "iopub.status.busy": "2023-06-05T22:50:32.272688Z",
     "iopub.status.idle": "2023-06-05T22:50:32.282210Z",
     "shell.execute_reply": "2023-06-05T22:50:32.281424Z"
    },
    "papermill": {
     "duration": 0.024502,
     "end_time": "2023-06-05T22:50:32.284568",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.260066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hedwig ( john cameron mitchell ) was born a bo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one of the more unusual and suggestively viole...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you get when you combine clueless and ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&gt;from the man who presented us with henry : th...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tibet has entered the american consciousness s...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content Label\n",
       "0  hedwig ( john cameron mitchell ) was born a bo...   pos\n",
       "1  one of the more unusual and suggestively viole...   pos\n",
       "2  what do you get when you combine clueless and ...   pos\n",
       "3  >from the man who presented us with henry : th...   pos\n",
       "4  tibet has entered the american consciousness s...   pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295eb992",
   "metadata": {
    "papermill": {
     "duration": 0.010849,
     "end_time": "2023-06-05T22:50:32.306870",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.296021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looks the same as the testing set, so that is great news. Let's see how the website has split the training and testing data. Usually, we hold back around 20% of the data for testing, and use the other 80% to actually train the model. Here we print the `.shape` of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127cd7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.331582Z",
     "iopub.status.busy": "2023-06-05T22:50:32.330956Z",
     "iopub.status.idle": "2023-06-05T22:50:32.336311Z",
     "shell.execute_reply": "2023-06-05T22:50:32.335262Z"
    },
    "papermill": {
     "duration": 0.020188,
     "end_time": "2023-06-05T22:50:32.338495",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.318307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 2)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(reviews_training.shape)\n",
    "print(reviews_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c414b71",
   "metadata": {
    "papermill": {
     "duration": 0.011423,
     "end_time": "2023-06-05T22:50:32.361309",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.349886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looks like there are 1800 training objects, and 200 testing objects. We did read earlier that there were 2000 total objects, so this checks out. 1800 is 90% of 2000, so the training set contains 90% of the data and the testing set contains the other 10%. This is alright, so we will proceed.\n",
    "\n",
    "The metadata said there were 1000 positive and 1000 negative reviews. It is important that there is a fairly equal distribution of classes in our training set so we do not create a biased model, and we would like to see a good distribution in our testing set as well so we know how well the model performs on each type. To check this, we will use the `.value_counts()` method from pandas to print out all unique values in a column and how many times they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d9a0f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.386112Z",
     "iopub.status.busy": "2023-06-05T22:50:32.385384Z",
     "iopub.status.idle": "2023-06-05T22:50:32.400912Z",
     "shell.execute_reply": "2023-06-05T22:50:32.399734Z"
    },
    "papermill": {
     "duration": 0.030521,
     "end_time": "2023-06-05T22:50:32.403220",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.372699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos    900\n",
      "neg    900\n",
      "Name: Label, dtype: int64\n",
      "pos    100\n",
      "neg    100\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reviews_training['Label'].value_counts())\n",
    "print(reviews_test['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293707e2",
   "metadata": {
    "papermill": {
     "duration": 0.011238,
     "end_time": "2023-06-05T22:50:32.426164",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.414926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great! Both training and testing sets are split exactly evenly with positive and negative examples. Now that we have a good grasp on the data, we should move on to preprocessing the data.\n",
    "\n",
    "## Preprocessing the data\n",
    "\n",
    "Preprocessing of natural language data is extremely important. How we transform the words into data arrays that a machine learning model can learn from will directly determine the success or failure of a model. For this workflow, we will focus on the 'bag-of-words' model for transforming the movie reviews into data arrays.\n",
    "\n",
    "The 'bag-of-words' model will represent each review as a collection (or bag) of individual words, without regard to their order. Each review has a giant array associated with it, where a column represents a potential word that could in that review (called the vocabulary). If a review contains that word once, it will be assigned a 1. If a review contains that word twice, that column will have a 2, and so on. In this way, you can represent every review by simply tallying up what words appear in it (called 'term frequency', or TF) and what words in the vocabulary don't appear.\n",
    "\n",
    "There is a problem associated with this strategy. There are a lot of words in English that appear quite often, and don't really determine the positive or negative sentiment of a review. Words like \"the\", \"of\", or \"and\". To make sure these words don't confuse and overcomplicate the model, we do a inverse document frequency calculation (IDF). This calculation gives a large weight (or importance) to words that appear less frequently in all the reviews, and common words are given a low weight. By combining the term frequency within a document (TF) and the IDF score, you can assign a high weight to words in reviews that are both frequent and unique. This method will allow us to focus on the words that make a much bigger difference in the overall sentiment.\n",
    "\n",
    "There is a tool that does all of these calculations for us! It is the `TfidfVectorizer` from sci-kit learn. With a large vocabulary, the arrays can be rather large, so the `TfidfVectorizer` outputs the high-dimensional vector in a format called a \"compressed sparse row matrix\". You can learn more about that from [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).\n",
    "\n",
    "Here we import `TfidfVectorizer` and initialize it with some hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b413574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:32.450527Z",
     "iopub.status.busy": "2023-06-05T22:50:32.450146Z",
     "iopub.status.idle": "2023-06-05T22:50:33.747797Z",
     "shell.execute_reply": "2023-06-05T22:50:33.746563Z"
    },
    "papermill": {
     "duration": 1.31313,
     "end_time": "2023-06-05T22:50:33.750540",
     "exception": false,
     "start_time": "2023-06-05T22:50:32.437410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1, max_df = 1.0, sublinear_tf = True, use_idf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e281e",
   "metadata": {
    "papermill": {
     "duration": 0.011263,
     "end_time": "2023-06-05T22:50:33.773409",
     "exception": false,
     "start_time": "2023-06-05T22:50:33.762146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`min_df` and `max_df` represent the minimum and maximum frequency threshold a word must have to be included in the vocabulary. With a `min_df` of 1, a word must appear at least once to be included. With the `max_df` set at 1.0 (meaning 100%), it means a word can appear as many times as it wants and will still be included in the vocabulary. When `sublinear_tf` is set to True, it will apply a log transformation to the term frequency scaling. This will greatly reduce the impact of those very frequent terms in each review, that might not carry significant information. Lastly, the `use_idf` parameter determines whether it will use inverse document frequency weighting at all, as discussed above. There are more hyperparameters that can be customized, and that information is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "Now with the vectorizer initialized, we are reading to transform our data. We first use the method `fit_transform` on our training data to create the vocabulary based on the hyperparameters we set, and then call on the `transform` method to use that same vocabulary to transform the testing data. At the end we print out the training_vectors to take a look at the compressed sparse row matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89a47db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:33.798136Z",
     "iopub.status.busy": "2023-06-05T22:50:33.797687Z",
     "iopub.status.idle": "2023-06-05T22:50:35.506867Z",
     "shell.execute_reply": "2023-06-05T22:50:35.505654Z"
    },
    "papermill": {
     "duration": 1.724448,
     "end_time": "2023-06-05T22:50:35.509332",
     "exception": false,
     "start_time": "2023-06-05T22:50:33.784884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 37728)\t0.025100981093704258\n",
      "  (0, 3591)\t0.023889006401408175\n",
      "  (0, 12297)\t0.04736892526046346\n",
      "  (0, 17656)\t0.04679663303327439\n",
      "  (0, 12346)\t0.0623848771840738\n",
      "  (0, 27323)\t0.045859217752151536\n",
      "  (0, 15767)\t0.043967339462022845\n",
      "  (0, 29509)\t0.03965913358706709\n",
      "  (0, 22330)\t0.046251603379063845\n",
      "  (0, 29250)\t0.0892927449327645\n",
      "  (0, 10870)\t0.0623848771840738\n",
      "  (0, 8393)\t0.0499960511075312\n",
      "  (0, 28411)\t0.025312511323553593\n",
      "  (0, 9454)\t0.03455505730553403\n",
      "  (0, 35929)\t0.053920525510464334\n",
      "  (0, 6599)\t0.04152331287466379\n",
      "  (0, 7739)\t0.058303281874462265\n",
      "  (0, 10361)\t0.07672081959651322\n",
      "  (0, 9320)\t0.039218946256633525\n",
      "  (0, 29172)\t0.06787285235103453\n",
      "  (0, 36072)\t0.0447568565992461\n",
      "  (0, 14950)\t0.06549673966980514\n",
      "  (0, 14344)\t0.024648936483590747\n",
      "  (0, 33376)\t0.039436923506143674\n",
      "  (0, 37056)\t0.026312347552146977\n",
      "  :\t:\n",
      "  (1799, 27189)\t0.027477323513756653\n",
      "  (1799, 15413)\t0.01749213768440475\n",
      "  (1799, 2440)\t0.029040213738001894\n",
      "  (1799, 26764)\t0.055525776455428374\n",
      "  (1799, 25356)\t0.07623040421427928\n",
      "  (1799, 36259)\t0.025756280753632375\n",
      "  (1799, 33941)\t0.04634051183776799\n",
      "  (1799, 31335)\t0.020647028559450793\n",
      "  (1799, 15854)\t0.028540304508983087\n",
      "  (1799, 37156)\t0.028780962566256764\n",
      "  (1799, 692)\t0.03320930780483834\n",
      "  (1799, 37243)\t0.05110694312913969\n",
      "  (1799, 37372)\t0.015341974972233734\n",
      "  (1799, 17888)\t0.024120884288193062\n",
      "  (1799, 1715)\t0.05081564064423462\n",
      "  (1799, 34241)\t0.04941770507510014\n",
      "  (1799, 17873)\t0.04677476958136358\n",
      "  (1799, 33829)\t0.05814834773569191\n",
      "  (1799, 31223)\t0.032771405761829485\n",
      "  (1799, 17829)\t0.025349208870225813\n",
      "  (1799, 33826)\t0.055367144243988226\n",
      "  (1799, 12635)\t0.052141108543857326\n",
      "  (1799, 29750)\t0.0538377205316027\n",
      "  (1799, 37092)\t0.04392989622496912\n",
      "  (1799, 16893)\t0.05343248177223278\n"
     ]
    }
   ],
   "source": [
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "print(training_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09a191",
   "metadata": {
    "papermill": {
     "duration": 0.01122,
     "end_time": "2023-06-05T22:50:35.532277",
     "exception": false,
     "start_time": "2023-06-05T22:50:35.521057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see parts of the first and last element. You can see each review number and index of the word in the vocabulary set in parenthesis, and on the right is the weight that the TF-IDF calculations assigned that word in that review. We are now ready to pass these vectors into our linear classifier and train our model!\n",
    "\n",
    "## Training the model\n",
    "\n",
    "We can now use our data to train a linear classifier. Let's use a support vector machine classifier. We import one from sci-kit learn, initialize it with the default hyperparameters, and call on the `.fit` method to train on the newly created vectors and the labels from our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242f696a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:35.558129Z",
     "iopub.status.busy": "2023-06-05T22:50:35.557666Z",
     "iopub.status.idle": "2023-06-05T22:50:47.720058Z",
     "shell.execute_reply": "2023-06-05T22:50:47.719093Z"
    },
    "papermill": {
     "duration": 12.17879,
     "end_time": "2023-06-05T22:50:47.722549",
     "exception": false,
     "start_time": "2023-06-05T22:50:35.543759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd8099",
   "metadata": {
    "papermill": {
     "duration": 0.011897,
     "end_time": "2023-06-05T22:50:47.746462",
     "exception": false,
     "start_time": "2023-06-05T22:50:47.734565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our model is now trained! Time to evaluate the model.\n",
    "\n",
    "## Testing the model\n",
    "\n",
    "Before we can start using the model confidently, we need to evaluate how well it performs. First, let's use the `.predict` method of our SVM classifier to predict on the testing vectors we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2dab8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:47.772440Z",
     "iopub.status.busy": "2023-06-05T22:50:47.772023Z",
     "iopub.status.idle": "2023-06-05T22:50:49.243266Z",
     "shell.execute_reply": "2023-06-05T22:50:49.241973Z"
    },
    "papermill": {
     "duration": 1.487658,
     "end_time": "2023-06-05T22:50:49.246092",
     "exception": false,
     "start_time": "2023-06-05T22:50:47.758434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(testing_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8038c4",
   "metadata": {
    "papermill": {
     "duration": 0.011695,
     "end_time": "2023-06-05T22:50:49.269722",
     "exception": false,
     "start_time": "2023-06-05T22:50:49.258027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have our predictions ready, let's use some metrics from sci-kit learn. We will try both `classification_report` and the common `accuracy_score`. We import both and give them the true labels for our testing set and the predictions we made. For `classification_report`, we specify the output format as a dictionary. This method will print out a report of some common metrics used to evaluate models including the precision, recall, F1-score, and how many samples were tested for each class. `accuracy_score` will just give us the simple percentage of how many predictions were correct out of all attempts. We print out each report for the 'pos' class and the 'neg' class. We then print the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e46542c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:49.296059Z",
     "iopub.status.busy": "2023-06-05T22:50:49.295665Z",
     "iopub.status.idle": "2023-06-05T22:50:49.325879Z",
     "shell.execute_reply": "2023-06-05T22:50:49.324316Z"
    },
    "papermill": {
     "duration": 0.046945,
     "end_time": "2023-06-05T22:50:49.329083",
     "exception": false,
     "start_time": "2023-06-05T22:50:49.282138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 100}\n",
      "Negatives:  {'precision': 0.9, 'recall': 0.9, 'f1-score': 0.9, 'support': 100}\n",
      "Overall accuracy:  0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8a9a5",
   "metadata": {
    "papermill": {
     "duration": 0.011899,
     "end_time": "2023-06-05T22:50:49.353118",
     "exception": false,
     "start_time": "2023-06-05T22:50:49.341219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We achieved 90% for the precision, recall, F1-score, and overall accuracy! There was a great balance to our results, meaning our model has the same levels of accuracy for both negative and positive movie reviews. Let's go back and adjust some of those hyperparameters to see how the accuracy of the model changes. We will copy the exact same code as before, but change some hyperparameters.\n",
    "\n",
    "## Experimenting with hyperparameters\n",
    "\n",
    "Hyperparameters let us fine-tune the model to our particular application. Adjusting and experimenting is a vital part to creating the best model possible. Let's start with turning off the sublinear log scaling we are doing to our term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467933d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:50:49.379248Z",
     "iopub.status.busy": "2023-06-05T22:50:49.378858Z",
     "iopub.status.idle": "2023-06-05T22:51:04.026098Z",
     "shell.execute_reply": "2023-06-05T22:51:04.024520Z"
    },
    "papermill": {
     "duration": 14.663677,
     "end_time": "2023-06-05T22:51:04.028724",
     "exception": false,
     "start_time": "2023-06-05T22:50:49.365047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.8571428571428571, 'recall': 0.84, 'f1-score': 0.8484848484848485, 'support': 100}\n",
      "Negatives:  {'precision': 0.8431372549019608, 'recall': 0.86, 'f1-score': 0.8514851485148515, 'support': 100}\n",
      "Overall accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 1, max_df = 1.0, sublinear_tf = False, use_idf = True)\n",
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])\n",
    "\n",
    "predictions = classifier.predict(testing_vectors)\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7fe9f",
   "metadata": {
    "papermill": {
     "duration": 0.012046,
     "end_time": "2023-06-05T22:51:04.052833",
     "exception": false,
     "start_time": "2023-06-05T22:51:04.040787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "All of our scores decreased! Looks like taking weight away from those frequent words really was helping the model. Let's change that back to 'True'. Next, let's experiment with increasing the min_df to 10. Maybe the very unique words are making the model too complicated, and removing those words from the vocabulary would help generalize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c11939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:04.079519Z",
     "iopub.status.busy": "2023-06-05T22:51:04.079127Z",
     "iopub.status.idle": "2023-06-05T22:51:17.064923Z",
     "shell.execute_reply": "2023-06-05T22:51:17.063768Z"
    },
    "papermill": {
     "duration": 13.002383,
     "end_time": "2023-06-05T22:51:17.067311",
     "exception": false,
     "start_time": "2023-06-05T22:51:04.064928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.9175257731958762, 'recall': 0.89, 'f1-score': 0.9035532994923858, 'support': 100}\n",
      "Negatives:  {'precision': 0.8932038834951457, 'recall': 0.92, 'f1-score': 0.9064039408866995, 'support': 100}\n",
      "Overall accuracy:  0.905\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 10, max_df = 1.0, sublinear_tf = True, use_idf = True)\n",
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])\n",
    "\n",
    "predictions = classifier.predict(testing_vectors)\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab2e6",
   "metadata": {
    "papermill": {
     "duration": 0.011998,
     "end_time": "2023-06-05T22:51:17.091499",
     "exception": false,
     "start_time": "2023-06-05T22:51:17.079501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There was a very slight increase in accuracy, but it did not seem to help the model much. Increasing min_df to more than 10 caused a decrease in the accuracy as well. I will set it back to 1 for now. Let's try increasing the max_df parameter to take away the most frequent words from the vocabulary, and see if that helps. Let's set it to 0.8, which means if a word is in more than 80% of the reviews it will not be incuded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a9eb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:17.117767Z",
     "iopub.status.busy": "2023-06-05T22:51:17.117388Z",
     "iopub.status.idle": "2023-06-05T22:51:30.968921Z",
     "shell.execute_reply": "2023-06-05T22:51:30.967660Z"
    },
    "papermill": {
     "duration": 13.868108,
     "end_time": "2023-06-05T22:51:30.971709",
     "exception": false,
     "start_time": "2023-06-05T22:51:17.103601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.898989898989899, 'recall': 0.89, 'f1-score': 0.8944723618090452, 'support': 100}\n",
      "Negatives:  {'precision': 0.8910891089108911, 'recall': 0.9, 'f1-score': 0.8955223880597015, 'support': 100}\n",
      "Overall accuracy:  0.895\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 1, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])\n",
    "\n",
    "predictions = classifier.predict(testing_vectors)\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b615a",
   "metadata": {
    "papermill": {
     "duration": 0.012239,
     "end_time": "2023-06-05T22:51:30.996378",
     "exception": false,
     "start_time": "2023-06-05T22:51:30.984139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This also does not seem to help! I performed further iterations, trying percentages closer to 100, and it did increase the performance of the model. This makes sense, because the most common words are already being weighted near 0 because of the log scaling of the term frequencies and inverse document frequency calculations. Lastly, let's try turning off the inverse document frequency calculation to see how important that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c972a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:31.023050Z",
     "iopub.status.busy": "2023-06-05T22:51:31.022604Z",
     "iopub.status.idle": "2023-06-05T22:51:44.285520Z",
     "shell.execute_reply": "2023-06-05T22:51:44.282271Z"
    },
    "papermill": {
     "duration": 13.279723,
     "end_time": "2023-06-05T22:51:44.288393",
     "exception": false,
     "start_time": "2023-06-05T22:51:31.008670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.8979591836734694, 'recall': 0.88, 'f1-score': 0.888888888888889, 'support': 100}\n",
      "Negatives:  {'precision': 0.8823529411764706, 'recall': 0.9, 'f1-score': 0.8910891089108911, 'support': 100}\n",
      "Overall accuracy:  0.89\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 1, max_df = 1.0, sublinear_tf = True, use_idf = False)\n",
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])\n",
    "\n",
    "predictions = classifier.predict(testing_vectors)\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c40c1",
   "metadata": {
    "papermill": {
     "duration": 0.012162,
     "end_time": "2023-06-05T22:51:44.313301",
     "exception": false,
     "start_time": "2023-06-05T22:51:44.301139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our accuracy did decrease, but not by too much! With some experimenting done, let's set the model back to the hyperparameters that achieved the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b383dd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:44.341488Z",
     "iopub.status.busy": "2023-06-05T22:51:44.340863Z",
     "iopub.status.idle": "2023-06-05T22:51:57.328944Z",
     "shell.execute_reply": "2023-06-05T22:51:57.327579Z"
    },
    "papermill": {
     "duration": 13.004839,
     "end_time": "2023-06-05T22:51:57.331635",
     "exception": false,
     "start_time": "2023-06-05T22:51:44.326796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives:  {'precision': 0.9175257731958762, 'recall': 0.89, 'f1-score': 0.9035532994923858, 'support': 100}\n",
      "Negatives:  {'precision': 0.8932038834951457, 'recall': 0.92, 'f1-score': 0.9064039408866995, 'support': 100}\n",
      "Overall accuracy:  0.905\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 10, max_df = 1.0, sublinear_tf = True, use_idf = True)\n",
    "training_vectors = vectorizer.fit_transform(reviews_training['Content'])\n",
    "testing_vectors = vectorizer.transform(reviews_test['Content'])\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(training_vectors, reviews_training['Label'])\n",
    "\n",
    "predictions = classifier.predict(testing_vectors)\n",
    "report = classification_report(reviews_test[\"Label\"],predictions,output_dict=True)\n",
    "accuracy = accuracy_score(reviews_test[\"Label\"], predictions)\n",
    "\n",
    "print('Positives: ', report['pos'])\n",
    "print('Negatives: ', report['neg'])\n",
    "print(\"Overall accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333995cf",
   "metadata": {
    "papermill": {
     "duration": 0.012638,
     "end_time": "2023-06-05T22:51:57.356840",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.344202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are now ready to use our model on some new movie reviews we can write ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5c58b",
   "metadata": {
    "papermill": {
     "duration": 0.012249,
     "end_time": "2023-06-05T22:51:57.381668",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.369419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using our model on new movie reviews\n",
    "\n",
    "Now it's time to try the model on some new movie reviews that we will write! I wrote one short positive review, used the same vectorizer to transform it, and then printed out the prediction the model made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f19dcfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:57.408917Z",
     "iopub.status.busy": "2023-06-05T22:51:57.408234Z",
     "iopub.status.idle": "2023-06-05T22:51:57.418598Z",
     "shell.execute_reply": "2023-06-05T22:51:57.417708Z"
    },
    "papermill": {
     "duration": 0.02662,
     "end_time": "2023-06-05T22:51:57.420747",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.394127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this review is  ['pos']\n"
     ]
    }
   ],
   "source": [
    "new_review = \"Although at times dragging with mundane realism, Sweeney managed to utilize these awkward gaps and conversations to captivate us with emotion\"\n",
    "review_vector = vectorizer.transform([new_review])\n",
    "print(\"The sentiment of this review is \", classifier.predict(review_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfbd5c",
   "metadata": {
    "papermill": {
     "duration": 0.012287,
     "end_time": "2023-06-05T22:51:57.445602",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.433315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Impressive! The model managed to predict the positive sentiment of my own movie review. Let's try a negative one and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b0b502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T22:51:57.472887Z",
     "iopub.status.busy": "2023-06-05T22:51:57.472202Z",
     "iopub.status.idle": "2023-06-05T22:51:57.482796Z",
     "shell.execute_reply": "2023-06-05T22:51:57.481619Z"
    },
    "papermill": {
     "duration": 0.02708,
     "end_time": "2023-06-05T22:51:57.485231",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.458151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this review is  ['neg']\n"
     ]
    }
   ],
   "source": [
    "another_review = 'Although once novel in its inception, it fails to evolve beyond the familiar tropes and pit-falls of its genre'\n",
    "review_vector = vectorizer.transform([another_review])\n",
    "print(\"The sentiment of this review is \", classifier.predict(review_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387cac1",
   "metadata": {
    "papermill": {
     "duration": 0.012379,
     "end_time": "2023-06-05T22:51:57.510240",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.497861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model succeeded once again! These were rather simple reviews, but I am very happy with the model's performance here. Go ahead and modify these reviews and try writing your own. See how it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ad7da",
   "metadata": {
    "papermill": {
     "duration": 0.012256,
     "end_time": "2023-06-05T22:51:57.534977",
     "exception": false,
     "start_time": "2023-06-05T22:51:57.522721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this workbook we used a data set of 2000 movie reviews to train a model to perform sentiment analysis. We wanted to create a model that could automatically determine whether a review was positive or negative. We used the 'TfidfVectorizer' to vectorize each review and feed it into a support vector machine classifier. We achieved 90% accuracy, and tested the model on our own reviews.\n",
    "\n",
    "We discovered that log scaling of the term frequencies helped the model ignore the more common words and achieve better performance. We also saw that inverse document frequency weighting was important for the same reasons, and when combined achieved the best performance. For further testing, we should find another movie data set that is labeled and test our model with that. This is a very common starting use case in learning natural langauge processing and sentiment analysis, and I am confident there are many data sets available. With new data, we can further fine-tune the model hyperparameters. We should also experiment with different linear classifiers, and there hyperparameters. For this notebook, I wanted to focus exclusively on the TfidfVectorizer parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 100.265135,
   "end_time": "2023-06-05T22:51:58.570302",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-05T22:50:18.305167",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
