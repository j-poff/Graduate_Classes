{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc74bd4",
   "metadata": {
    "papermill": {
     "duration": 0.011534,
     "end_time": "2023-05-29T04:19:45.098611",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.087077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides you an opportunity to demonstrate proficiency in meeting course learning goals by applying a support vector machine to solve a classification problem using widely-used ML libraries and an ML workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df68326",
   "metadata": {
    "papermill": {
     "duration": 0.010023,
     "end_time": "2023-05-29T04:19:45.119096",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.109073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mine Detection (Revisited)\n",
    "\n",
    "In this notebook, you will revisit [a previously seen classification problem](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons), and see if you can build a better classification model that can predict whether or not a sonar signature is from a mine or a rock.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Tip:</b> We suggest reviewing your Notebook 4: Classification with Perceptrons.\n",
    "</div>\n",
    "\n",
    "We'll use a version of the [sonar data set](https://www.openml.org/search?type=data&sort=runs&id=40&status=active) by Gorman and Sejnowski. Take a moment now to [reacquaint yourself with the subject matter of this data set](https://datahub.io/machine-learning/sonar%23resource-sonar), and look at the details of the version of this data set, [Mines vs Rocks, hosted on Kaggle](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks).\n",
    "\n",
    "Similar to [a previous notebook](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons), this notebook expects each student to implement the ML workflow steps. We will get you started by providing the first step, loading the data, and providing some landmarks and tips below. Your process should demonstrate:\n",
    "\n",
    "1. Loading the data\n",
    "2. Exploring the data\n",
    "3. Preprocessing the data\n",
    "4. Preparing the training and test sets\n",
    "5. Creating and configuring a sklearn.svm.SVC\n",
    "6. Training the SVM\n",
    "7. Validating and Testing the SVM\n",
    "8. Demonstrating making predictions\n",
    "9. Evaluate (and Improve) the results\n",
    "\n",
    "Can you train a classifier that can predict whether a sonar signature is from a mine or a rock? \"Three trained human subjects were each tested on 100 signals, chosen at random from the set of 208 returns used to create this data set. Their responses ranged between 88% and 97% correct.\" Can your classifier outperform the human subjects?\n",
    "\n",
    "Most importantly, how does the performance of the SVM classifier compare to the perceptron results observed in [Notebook 4](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a09234",
   "metadata": {
    "papermill": {
     "duration": 0.00981,
     "end_time": "2023-05-29T04:19:45.139227",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.129417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "The notebook comes pre-bundled with the [Mines vs Rocks data set](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks). Our first step is to create a pandas DataFrame from the CSV file. Note that the CSV file has no header row. Loading the CSV file into a DataFrame will make it easy for us to explore the data, preprocess it, and split it into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0df938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.161909Z",
     "iopub.status.busy": "2023-05-29T04:19:45.161453Z",
     "iopub.status.idle": "2023-05-29T04:19:45.196489Z",
     "shell.execute_reply": "2023-05-29T04:19:45.195191Z"
    },
    "papermill": {
     "duration": 0.050177,
     "end_time": "2023-05-29T04:19:45.199707",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.149530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sonar_csv_path = \"../input/mines-vs-rocks/sonar.all-data.csv\"\n",
    "sonar_data = pd.read_csv(sonar_csv_path, header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516d5ef",
   "metadata": {
    "papermill": {
     "duration": 0.010503,
     "end_time": "2023-05-29T04:19:45.220840",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.210337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now have a pandas DataFrame encapsulating the sonar data, and can proceed with our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad5fce",
   "metadata": {
    "papermill": {
     "duration": 0.009854,
     "end_time": "2023-05-29T04:19:45.241003",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.231149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825f52e",
   "metadata": {
    "papermill": {
     "duration": 0.009857,
     "end_time": "2023-05-29T04:19:45.260946",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.251089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at the data using the `.head()` method. This will give us the dimensions, and a good look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e788436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.284766Z",
     "iopub.status.busy": "2023-05-29T04:19:45.283912Z",
     "iopub.status.idle": "2023-05-29T04:19:45.332009Z",
     "shell.execute_reply": "2023-05-29T04:19:45.330734Z"
    },
    "papermill": {
     "duration": 0.062861,
     "end_time": "2023-05-29T04:19:45.335007",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.272146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef11350",
   "metadata": {
    "papermill": {
     "duration": 0.010296,
     "end_time": "2023-05-29T04:19:45.356059",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.345763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see we have 61 columns, which is what we expect based on the documentation on the kaggle website where the data came from. The first 60 columns are the features, and the last one is the labels. The first five are all \"R\" for rock, but we would expect some others would be \"M\" for mine. \n",
    "\n",
    "Next, let's use the `.describe` method to get some statistics on the features to see the range they occupy. We first set some pandas options so it will display all of the columns instead of hiding the middle ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137e40a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.379405Z",
     "iopub.status.busy": "2023-05-29T04:19:45.378994Z",
     "iopub.status.idle": "2023-05-29T04:19:45.584708Z",
     "shell.execute_reply": "2023-05-29T04:19:45.583457Z"
    },
    "papermill": {
     "duration": 0.221049,
     "end_time": "2023-05-29T04:19:45.587680",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.366631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.249050</td>\n",
       "      <td>0.263950</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.308400</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.434950</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.745600</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>0.490350</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.351050</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.321150</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.278050</td>\n",
       "      <td>0.259500</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.222550</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.121350</td>\n",
       "      <td>0.101650</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.734200</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965700</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259    0.236013    0.250221   \n",
       "std      0.061788    0.085152    0.118387    0.134416    0.132705    0.140072   \n",
       "min      0.003300    0.005500    0.007500    0.011300    0.028900    0.023600   \n",
       "25%      0.080900    0.080425    0.097025    0.111275    0.129250    0.133475   \n",
       "50%      0.106950    0.112100    0.152250    0.182400    0.224800    0.249050   \n",
       "75%      0.154000    0.169600    0.233425    0.268700    0.301650    0.331250   \n",
       "max      0.372900    0.459000    0.682800    0.710600    0.734200    0.706000   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.273305    0.296568    0.320201    0.378487    0.415983    0.452318   \n",
       "std      0.140962    0.164474    0.205427    0.232650    0.263677    0.261529   \n",
       "min      0.018400    0.027300    0.003100    0.016200    0.034900    0.037500   \n",
       "25%      0.166125    0.175175    0.164625    0.196300    0.205850    0.242075   \n",
       "50%      0.263950    0.281100    0.281700    0.304700    0.308400    0.368300   \n",
       "75%      0.351250    0.386175    0.452925    0.535725    0.659425    0.679050   \n",
       "max      0.713100    0.997000    1.000000    0.998800    1.000000    1.000000   \n",
       "\n",
       "               18          19          20          21          22          23  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504812    0.563047    0.609060    0.624275    0.646975    0.672654   \n",
       "std      0.257988    0.262653    0.257818    0.255883    0.250175    0.239116   \n",
       "min      0.049400    0.065600    0.051200    0.021900    0.056300    0.023900   \n",
       "25%      0.299075    0.350625    0.399725    0.406925    0.450225    0.540725   \n",
       "50%      0.434950    0.542500    0.617700    0.664900    0.699700    0.698500   \n",
       "75%      0.731400    0.809325    0.816975    0.831975    0.848575    0.872175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               24          25          26          27          28          29  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.675424    0.699866    0.702155    0.694024    0.642074    0.580928   \n",
       "std      0.244926    0.237228    0.245657    0.237189    0.240250    0.220749   \n",
       "min      0.024000    0.092100    0.048100    0.028400    0.014400    0.061300   \n",
       "25%      0.525800    0.544175    0.531900    0.534775    0.463700    0.411400   \n",
       "50%      0.721100    0.754500    0.745600    0.731900    0.680800    0.607150   \n",
       "75%      0.873725    0.893800    0.917100    0.900275    0.852125    0.735175   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               30          31          32          33          34          35  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.504475    0.439040    0.417220    0.403233    0.392571    0.384848   \n",
       "std      0.213992    0.213237    0.206513    0.231242    0.259132    0.264121   \n",
       "min      0.048200    0.040400    0.047700    0.021200    0.022300    0.008000   \n",
       "25%      0.345550    0.281400    0.257875    0.217575    0.179375    0.154350   \n",
       "50%      0.490350    0.429600    0.391200    0.351050    0.312750    0.321150   \n",
       "75%      0.641950    0.580300    0.556125    0.596125    0.593350    0.556525   \n",
       "max      0.965700    0.930600    1.000000    0.964700    1.000000    1.000000   \n",
       "\n",
       "               36          37          38          39          40          41  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.363807    0.339657    0.325800    0.311207    0.289252    0.278293   \n",
       "std      0.239912    0.212973    0.199075    0.178662    0.171111    0.168728   \n",
       "min      0.035100    0.038300    0.037100    0.011700    0.036000    0.005600   \n",
       "25%      0.160100    0.174275    0.173975    0.186450    0.163100    0.158900   \n",
       "50%      0.306300    0.312700    0.283500    0.278050    0.259500    0.245100   \n",
       "75%      0.518900    0.440550    0.434900    0.424350    0.387525    0.384250   \n",
       "max      0.949700    1.000000    0.985700    0.929700    0.899500    0.824600   \n",
       "\n",
       "               42          43          44          45          46          47  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.246542    0.214075    0.197232    0.160631    0.122453    0.091424   \n",
       "std      0.138993    0.133291    0.151628    0.133938    0.086953    0.062417   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.155200    0.126875    0.094475    0.068550    0.064250    0.045125   \n",
       "50%      0.222550    0.177700    0.148000    0.121350    0.101650    0.078100   \n",
       "75%      0.324525    0.271750    0.231550    0.200375    0.154425    0.120100   \n",
       "max      0.773300    0.776200    0.703400    0.729200    0.552200    0.333900   \n",
       "\n",
       "               48          49          50          51          52          53  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.051929    0.020424    0.016069    0.013420    0.010709    0.010941   \n",
       "std      0.035954    0.013665    0.012008    0.009634    0.007060    0.007301   \n",
       "min      0.000000    0.000000    0.000000    0.000800    0.000500    0.001000   \n",
       "25%      0.026350    0.011550    0.008425    0.007275    0.005075    0.005375   \n",
       "50%      0.044700    0.017900    0.013900    0.011400    0.009550    0.009300   \n",
       "75%      0.068525    0.025275    0.020825    0.016725    0.014900    0.014500   \n",
       "max      0.198100    0.082500    0.100400    0.070900    0.039000    0.035200   \n",
       "\n",
       "               54          55          56          57          58          59  \n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000  \n",
       "mean     0.009290    0.008222    0.007820    0.007949    0.007941    0.006507  \n",
       "std      0.007088    0.005736    0.005785    0.006470    0.006181    0.005031  \n",
       "min      0.000600    0.000400    0.000300    0.000300    0.000100    0.000600  \n",
       "25%      0.004150    0.004400    0.003700    0.003600    0.003675    0.003100  \n",
       "50%      0.007500    0.006850    0.005950    0.005800    0.006400    0.005300  \n",
       "75%      0.012100    0.010575    0.010425    0.010350    0.010325    0.008525  \n",
       "max      0.044700    0.039400    0.035500    0.044000    0.036400    0.043900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "sonar_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c01822",
   "metadata": {
    "papermill": {
     "duration": 0.011489,
     "end_time": "2023-05-29T04:19:45.611108",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.599619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The first thing we notice about this data is that it is not normalized. The minimum and maximum values are different for each feature, which can lead to features with larger numbers being misconstrued as more important by the machine learning algorithm. It is important to normalize this later on.\n",
    "\n",
    "Now that we have a good understanding of our features space, let's take another look at the labels and their distribution. We know that the labels are in column 60. We will use the `.value_counts()` method to see the distribution of values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba49c3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.636665Z",
     "iopub.status.busy": "2023-05-29T04:19:45.636201Z",
     "iopub.status.idle": "2023-05-29T04:19:45.645152Z",
     "shell.execute_reply": "2023-05-29T04:19:45.643859Z"
    },
    "papermill": {
     "duration": 0.024865,
     "end_time": "2023-05-29T04:19:45.647886",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.623021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M    111\n",
      "R     97\n",
      "Name: 60, dtype: int64\n",
      "Total objects: 208\n"
     ]
    }
   ],
   "source": [
    "label_distribution = sonar_data[60].value_counts()\n",
    "print(label_distribution)\n",
    "print(\"Total objects: \" + str(len(sonar_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b677c95",
   "metadata": {
    "papermill": {
     "duration": 0.011484,
     "end_time": "2023-05-29T04:19:45.671209",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.659725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 111 data objects labeled \"mine\", and 97 data objects labeled \"rock\". This is not evenly distributed, so balancing the label weights during training might be advantageous. This also means there are 208 total data objects, which we confirm with the len() function above.\n",
    "\n",
    "The next step is to make sure there are no null values I need to resolve. This is very simple with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50f4422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.697958Z",
     "iopub.status.busy": "2023-05-29T04:19:45.697183Z",
     "iopub.status.idle": "2023-05-29T04:19:45.705226Z",
     "shell.execute_reply": "2023-05-29T04:19:45.704367Z"
    },
    "papermill": {
     "duration": 0.024181,
     "end_time": "2023-05-29T04:19:45.707724",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.683543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d42bc",
   "metadata": {
    "papermill": {
     "duration": 0.011928,
     "end_time": "2023-05-29T04:19:45.731749",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.719821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are no null values, so it looks like everything is good to go. We are ready to start preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eebd4c",
   "metadata": {
    "papermill": {
     "duration": 0.011497,
     "end_time": "2023-05-29T04:19:45.755180",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.743683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Preprocess the Data\n",
    "\n",
    "First we import `StandardScaler` from sklearn, which normalizes each feature by its z-score (z = (x-u) / s, where u is the mean  and s is the standard deviation of each feature). Next, we split the features (X) from the labels (y) by slicing the pandas dataframe. Then, we initialize the scaler, scale it, and put the data back into a dataframe so we can use the `describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af22126d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:45.780889Z",
     "iopub.status.busy": "2023-05-29T04:19:45.780444Z",
     "iopub.status.idle": "2023-05-29T04:19:47.011342Z",
     "shell.execute_reply": "2023-05-29T04:19:47.010042Z"
    },
    "papermill": {
     "duration": 1.247091,
     "end_time": "2023-05-29T04:19:47.014202",
     "exception": false,
     "start_time": "2023-05-29T04:19:45.767111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "      <td>2.080000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.708035e-17</td>\n",
       "      <td>6.832142e-17</td>\n",
       "      <td>-1.195625e-16</td>\n",
       "      <td>1.622634e-16</td>\n",
       "      <td>-1.793437e-16</td>\n",
       "      <td>2.049643e-16</td>\n",
       "      <td>1.024821e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>-3.757678e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>-3.416071e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.832142e-17</td>\n",
       "      <td>-2.562053e-17</td>\n",
       "      <td>-5.551115e-17</td>\n",
       "      <td>1.281027e-17</td>\n",
       "      <td>2.604754e-16</td>\n",
       "      <td>-8.540177e-18</td>\n",
       "      <td>-9.394195e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.440892e-16</td>\n",
       "      <td>-1.195625e-16</td>\n",
       "      <td>-3.074464e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>-7.771561e-16</td>\n",
       "      <td>5.636517e-16</td>\n",
       "      <td>-2.647455e-16</td>\n",
       "      <td>-8.540177e-17</td>\n",
       "      <td>1.281027e-16</td>\n",
       "      <td>1.537232e-16</td>\n",
       "      <td>2.562053e-17</td>\n",
       "      <td>1.494531e-16</td>\n",
       "      <td>-5.978124e-17</td>\n",
       "      <td>1.708035e-17</td>\n",
       "      <td>1.024821e-16</td>\n",
       "      <td>1.195625e-16</td>\n",
       "      <td>2.732857e-16</td>\n",
       "      <td>-1.216975e-16</td>\n",
       "      <td>5.978124e-17</td>\n",
       "      <td>2.732857e-16</td>\n",
       "      <td>-1.494531e-16</td>\n",
       "      <td>-1.024821e-16</td>\n",
       "      <td>8.540177e-17</td>\n",
       "      <td>3.074464e-16</td>\n",
       "      <td>-3.416071e-17</td>\n",
       "      <td>1.708035e-16</td>\n",
       "      <td>1.366428e-16</td>\n",
       "      <td>2.391250e-16</td>\n",
       "      <td>-1.366428e-16</td>\n",
       "      <td>-3.074464e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>1.024821e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>-1.451830e-16</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>-2.391250e-16</td>\n",
       "      <td>3.416071e-17</td>\n",
       "      <td>-1.110223e-16</td>\n",
       "      <td>1.345078e-16</td>\n",
       "      <td>7.686159e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "      <td>1.002413e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.206158e+00</td>\n",
       "      <td>-1.150725e+00</td>\n",
       "      <td>-1.104253e+00</td>\n",
       "      <td>-1.036115e+00</td>\n",
       "      <td>-1.236093e+00</td>\n",
       "      <td>-1.600493e+00</td>\n",
       "      <td>-1.921613e+00</td>\n",
       "      <td>-1.522110e+00</td>\n",
       "      <td>-1.443689e+00</td>\n",
       "      <td>-1.468833e+00</td>\n",
       "      <td>-1.564472e+00</td>\n",
       "      <td>-1.621794</td>\n",
       "      <td>-1.812688e+00</td>\n",
       "      <td>-1.641094e+00</td>\n",
       "      <td>-1.547347e+00</td>\n",
       "      <td>-1.560974e+00</td>\n",
       "      <td>-1.448751e+00</td>\n",
       "      <td>-1.589952e+00</td>\n",
       "      <td>-1.769499e+00</td>\n",
       "      <td>-1.898502</td>\n",
       "      <td>-2.168994e+00</td>\n",
       "      <td>-2.359782e+00</td>\n",
       "      <td>-2.366740e+00</td>\n",
       "      <td>-2.719680e+00</td>\n",
       "      <td>-2.666087e+00</td>\n",
       "      <td>-2.568134e+00</td>\n",
       "      <td>-2.668895e+00</td>\n",
       "      <td>-2.813072e+00</td>\n",
       "      <td>-2.618893e+00</td>\n",
       "      <td>-2.359606e+00</td>\n",
       "      <td>-2.137348e+00</td>\n",
       "      <td>-1.873982e+00</td>\n",
       "      <td>-1.793647e+00</td>\n",
       "      <td>-1.656081e+00</td>\n",
       "      <td>-1.432336e+00</td>\n",
       "      <td>-1.430241e+00</td>\n",
       "      <td>-1.373417e+00</td>\n",
       "      <td>-1.418414e+00</td>\n",
       "      <td>-1.453707e+00</td>\n",
       "      <td>-1.680436e+00</td>\n",
       "      <td>-1.483614e+00</td>\n",
       "      <td>-1.620067e+00</td>\n",
       "      <td>-1.778046e+00</td>\n",
       "      <td>-1.609948e+00</td>\n",
       "      <td>-1.303898e+00</td>\n",
       "      <td>-1.202190e+00</td>\n",
       "      <td>-1.411667e+00</td>\n",
       "      <td>-1.468270e+00</td>\n",
       "      <td>-1.447799e+00</td>\n",
       "      <td>-1.498229e+00</td>\n",
       "      <td>-1.341343e+00</td>\n",
       "      <td>-1.313126e+00</td>\n",
       "      <td>-1.449472e+00</td>\n",
       "      <td>-1.364897e+00</td>\n",
       "      <td>-1.229092e+00</td>\n",
       "      <td>-1.366868e+00</td>\n",
       "      <td>-1.302971e+00</td>\n",
       "      <td>-1.185113e+00</td>\n",
       "      <td>-1.271603e+00</td>\n",
       "      <td>-1.176985e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.894939e-01</td>\n",
       "      <td>-6.686781e-01</td>\n",
       "      <td>-6.490624e-01</td>\n",
       "      <td>-6.359298e-01</td>\n",
       "      <td>-6.703975e-01</td>\n",
       "      <td>-6.367565e-01</td>\n",
       "      <td>-6.626732e-01</td>\n",
       "      <td>-6.400918e-01</td>\n",
       "      <td>-6.856590e-01</td>\n",
       "      <td>-7.232644e-01</td>\n",
       "      <td>-8.064569e-01</td>\n",
       "      <td>-0.835483</td>\n",
       "      <td>-7.621827e-01</td>\n",
       "      <td>-7.398484e-01</td>\n",
       "      <td>-7.591598e-01</td>\n",
       "      <td>-7.849821e-01</td>\n",
       "      <td>-7.988564e-01</td>\n",
       "      <td>-8.058388e-01</td>\n",
       "      <td>-7.993884e-01</td>\n",
       "      <td>-0.810706</td>\n",
       "      <td>-8.139075e-01</td>\n",
       "      <td>-8.514605e-01</td>\n",
       "      <td>-7.883456e-01</td>\n",
       "      <td>-5.530684e-01</td>\n",
       "      <td>-6.123656e-01</td>\n",
       "      <td>-6.578782e-01</td>\n",
       "      <td>-6.947312e-01</td>\n",
       "      <td>-6.730211e-01</td>\n",
       "      <td>-7.442437e-01</td>\n",
       "      <td>-7.698181e-01</td>\n",
       "      <td>-7.444604e-01</td>\n",
       "      <td>-7.410570e-01</td>\n",
       "      <td>-7.734584e-01</td>\n",
       "      <td>-8.048124e-01</td>\n",
       "      <td>-8.247161e-01</td>\n",
       "      <td>-8.748025e-01</td>\n",
       "      <td>-8.511364e-01</td>\n",
       "      <td>-7.784131e-01</td>\n",
       "      <td>-7.644914e-01</td>\n",
       "      <td>-6.999700e-01</td>\n",
       "      <td>-7.390302e-01</td>\n",
       "      <td>-7.093139e-01</td>\n",
       "      <td>-6.587522e-01</td>\n",
       "      <td>-6.557863e-01</td>\n",
       "      <td>-6.793257e-01</td>\n",
       "      <td>-6.891506e-01</td>\n",
       "      <td>-6.709773e-01</td>\n",
       "      <td>-7.435627e-01</td>\n",
       "      <td>-7.131495e-01</td>\n",
       "      <td>-6.509652e-01</td>\n",
       "      <td>-6.380641e-01</td>\n",
       "      <td>-6.394049e-01</td>\n",
       "      <td>-7.999231e-01</td>\n",
       "      <td>-7.642025e-01</td>\n",
       "      <td>-7.270112e-01</td>\n",
       "      <td>-6.678488e-01</td>\n",
       "      <td>-7.138771e-01</td>\n",
       "      <td>-6.738235e-01</td>\n",
       "      <td>-6.918580e-01</td>\n",
       "      <td>-6.788714e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.774703e-01</td>\n",
       "      <td>-2.322506e-01</td>\n",
       "      <td>-2.486515e-01</td>\n",
       "      <td>-2.120457e-01</td>\n",
       "      <td>-2.292089e-01</td>\n",
       "      <td>-2.106432e-01</td>\n",
       "      <td>-2.400524e-01</td>\n",
       "      <td>-2.672134e-01</td>\n",
       "      <td>-2.180558e-01</td>\n",
       "      <td>-1.928459e-01</td>\n",
       "      <td>-8.469964e-02</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>-6.652752e-02</td>\n",
       "      <td>-9.427355e-02</td>\n",
       "      <td>-1.878739e-01</td>\n",
       "      <td>-3.179220e-01</td>\n",
       "      <td>-4.089954e-01</td>\n",
       "      <td>-3.220326e-01</td>\n",
       "      <td>-2.714467e-01</td>\n",
       "      <td>-0.078416</td>\n",
       "      <td>3.359247e-02</td>\n",
       "      <td>1.591469e-01</td>\n",
       "      <td>2.112606e-01</td>\n",
       "      <td>1.083491e-01</td>\n",
       "      <td>1.869404e-01</td>\n",
       "      <td>2.308561e-01</td>\n",
       "      <td>1.772798e-01</td>\n",
       "      <td>1.600721e-01</td>\n",
       "      <td>1.615793e-01</td>\n",
       "      <td>1.190734e-01</td>\n",
       "      <td>-6.616850e-02</td>\n",
       "      <td>-4.437862e-02</td>\n",
       "      <td>-1.262995e-01</td>\n",
       "      <td>-2.262096e-01</td>\n",
       "      <td>-3.087757e-01</td>\n",
       "      <td>-2.417501e-01</td>\n",
       "      <td>-2.402772e-01</td>\n",
       "      <td>-1.268809e-01</td>\n",
       "      <td>-2.129934e-01</td>\n",
       "      <td>-1.860318e-01</td>\n",
       "      <td>-1.742944e-01</td>\n",
       "      <td>-1.972008e-01</td>\n",
       "      <td>-1.730277e-01</td>\n",
       "      <td>-2.735576e-01</td>\n",
       "      <td>-3.254731e-01</td>\n",
       "      <td>-2.939871e-01</td>\n",
       "      <td>-2.398208e-01</td>\n",
       "      <td>-2.139841e-01</td>\n",
       "      <td>-2.015434e-01</td>\n",
       "      <td>-1.851537e-01</td>\n",
       "      <td>-1.810370e-01</td>\n",
       "      <td>-2.102002e-01</td>\n",
       "      <td>-1.645716e-01</td>\n",
       "      <td>-2.252935e-01</td>\n",
       "      <td>-2.532164e-01</td>\n",
       "      <td>-2.396997e-01</td>\n",
       "      <td>-3.240352e-01</td>\n",
       "      <td>-3.329639e-01</td>\n",
       "      <td>-2.499546e-01</td>\n",
       "      <td>-2.405314e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.784345e-01</td>\n",
       "      <td>2.893335e-01</td>\n",
       "      <td>3.682681e-01</td>\n",
       "      <td>2.285353e-01</td>\n",
       "      <td>4.524231e-01</td>\n",
       "      <td>5.012417e-01</td>\n",
       "      <td>5.232608e-01</td>\n",
       "      <td>4.096773e-01</td>\n",
       "      <td>4.692723e-01</td>\n",
       "      <td>4.507410e-01</td>\n",
       "      <td>4.958032e-01</td>\n",
       "      <td>0.579876</td>\n",
       "      <td>5.542820e-01</td>\n",
       "      <td>5.461209e-01</td>\n",
       "      <td>6.476456e-01</td>\n",
       "      <td>6.774890e-01</td>\n",
       "      <td>9.254848e-01</td>\n",
       "      <td>8.690373e-01</td>\n",
       "      <td>8.804084e-01</td>\n",
       "      <td>0.939920</td>\n",
       "      <td>8.083857e-01</td>\n",
       "      <td>8.136570e-01</td>\n",
       "      <td>8.077787e-01</td>\n",
       "      <td>8.364219e-01</td>\n",
       "      <td>8.115900e-01</td>\n",
       "      <td>8.194722e-01</td>\n",
       "      <td>8.770920e-01</td>\n",
       "      <td>8.716615e-01</td>\n",
       "      <td>8.764117e-01</td>\n",
       "      <td>7.004289e-01</td>\n",
       "      <td>6.439769e-01</td>\n",
       "      <td>6.640521e-01</td>\n",
       "      <td>6.742455e-01</td>\n",
       "      <td>8.361697e-01</td>\n",
       "      <td>7.766817e-01</td>\n",
       "      <td>6.515635e-01</td>\n",
       "      <td>6.480175e-01</td>\n",
       "      <td>4.748773e-01</td>\n",
       "      <td>5.493604e-01</td>\n",
       "      <td>6.348106e-01</td>\n",
       "      <td>5.757088e-01</td>\n",
       "      <td>6.294876e-01</td>\n",
       "      <td>5.624103e-01</td>\n",
       "      <td>4.337440e-01</td>\n",
       "      <td>2.268742e-01</td>\n",
       "      <td>2.974485e-01</td>\n",
       "      <td>3.685825e-01</td>\n",
       "      <td>4.605360e-01</td>\n",
       "      <td>4.627081e-01</td>\n",
       "      <td>3.558478e-01</td>\n",
       "      <td>3.970293e-01</td>\n",
       "      <td>3.438640e-01</td>\n",
       "      <td>5.950106e-01</td>\n",
       "      <td>4.886751e-01</td>\n",
       "      <td>3.973675e-01</td>\n",
       "      <td>4.112618e-01</td>\n",
       "      <td>4.513169e-01</td>\n",
       "      <td>3.719959e-01</td>\n",
       "      <td>3.865486e-01</td>\n",
       "      <td>4.020352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.706053e+00</td>\n",
       "      <td>5.944643e+00</td>\n",
       "      <td>6.836142e+00</td>\n",
       "      <td>8.025419e+00</td>\n",
       "      <td>5.878863e+00</td>\n",
       "      <td>4.710224e+00</td>\n",
       "      <td>4.074573e+00</td>\n",
       "      <td>3.816498e+00</td>\n",
       "      <td>4.274237e+00</td>\n",
       "      <td>3.746234e+00</td>\n",
       "      <td>3.763162e+00</td>\n",
       "      <td>3.261740</td>\n",
       "      <td>3.127477e+00</td>\n",
       "      <td>4.268880e+00</td>\n",
       "      <td>3.317184e+00</td>\n",
       "      <td>2.672727e+00</td>\n",
       "      <td>2.220238e+00</td>\n",
       "      <td>2.099203e+00</td>\n",
       "      <td>1.924052e+00</td>\n",
       "      <td>1.667629</td>\n",
       "      <td>1.519998e+00</td>\n",
       "      <td>1.471889e+00</td>\n",
       "      <td>1.414514e+00</td>\n",
       "      <td>1.372284e+00</td>\n",
       "      <td>1.328397e+00</td>\n",
       "      <td>1.268223e+00</td>\n",
       "      <td>1.215369e+00</td>\n",
       "      <td>1.293121e+00</td>\n",
       "      <td>1.493402e+00</td>\n",
       "      <td>1.902987e+00</td>\n",
       "      <td>2.160531e+00</td>\n",
       "      <td>2.310789e+00</td>\n",
       "      <td>2.828812e+00</td>\n",
       "      <td>2.433911e+00</td>\n",
       "      <td>2.349744e+00</td>\n",
       "      <td>2.334674e+00</td>\n",
       "      <td>2.448005e+00</td>\n",
       "      <td>3.108070e+00</td>\n",
       "      <td>3.322838e+00</td>\n",
       "      <td>3.470167e+00</td>\n",
       "      <td>3.574989e+00</td>\n",
       "      <td>3.245601e+00</td>\n",
       "      <td>3.798951e+00</td>\n",
       "      <td>4.227453e+00</td>\n",
       "      <td>3.346264e+00</td>\n",
       "      <td>4.255258e+00</td>\n",
       "      <td>4.954231e+00</td>\n",
       "      <td>3.894165e+00</td>\n",
       "      <td>4.075316e+00</td>\n",
       "      <td>4.553653e+00</td>\n",
       "      <td>7.039574e+00</td>\n",
       "      <td>5.980752e+00</td>\n",
       "      <td>4.016680e+00</td>\n",
       "      <td>3.330819e+00</td>\n",
       "      <td>5.008027e+00</td>\n",
       "      <td>5.448568e+00</td>\n",
       "      <td>4.795888e+00</td>\n",
       "      <td>5.585599e+00</td>\n",
       "      <td>4.615037e+00</td>\n",
       "      <td>7.450343e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   1.708035e-17  6.832142e-17 -1.195625e-16  1.622634e-16 -1.793437e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.206158e+00 -1.150725e+00 -1.104253e+00 -1.036115e+00 -1.236093e+00   \n",
       "25%   -6.894939e-01 -6.686781e-01 -6.490624e-01 -6.359298e-01 -6.703975e-01   \n",
       "50%   -2.774703e-01 -2.322506e-01 -2.486515e-01 -2.120457e-01 -2.292089e-01   \n",
       "75%    2.784345e-01  2.893335e-01  3.682681e-01  2.285353e-01  4.524231e-01   \n",
       "max    4.706053e+00  5.944643e+00  6.836142e+00  8.025419e+00  5.878863e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   2.049643e-16  1.024821e-16  3.416071e-17 -3.757678e-16  3.416071e-17   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.600493e+00 -1.921613e+00 -1.522110e+00 -1.443689e+00 -1.468833e+00   \n",
       "25%   -6.367565e-01 -6.626732e-01 -6.400918e-01 -6.856590e-01 -7.232644e-01   \n",
       "50%   -2.106432e-01 -2.400524e-01 -2.672134e-01 -2.180558e-01 -1.928459e-01   \n",
       "75%    5.012417e-01  5.232608e-01  4.096773e-01  4.692723e-01  4.507410e-01   \n",
       "max    4.710224e+00  4.074573e+00  3.816498e+00  4.274237e+00  3.746234e+00   \n",
       "\n",
       "                 10          11            12            13            14  \\\n",
       "count  2.080000e+02  208.000000  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean  -3.416071e-17    0.000000  6.832142e-17 -2.562053e-17 -5.551115e-17   \n",
       "std    1.002413e+00    1.002413  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.564472e+00   -1.621794 -1.812688e+00 -1.641094e+00 -1.547347e+00   \n",
       "25%   -8.064569e-01   -0.835483 -7.621827e-01 -7.398484e-01 -7.591598e-01   \n",
       "50%   -8.469964e-02   -0.008381 -6.652752e-02 -9.427355e-02 -1.878739e-01   \n",
       "75%    4.958032e-01    0.579876  5.542820e-01  5.461209e-01  6.476456e-01   \n",
       "max    3.763162e+00    3.261740  3.127477e+00  4.268880e+00  3.317184e+00   \n",
       "\n",
       "                 15            16            17            18          19  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  208.000000   \n",
       "mean   1.281027e-17  2.604754e-16 -8.540177e-18 -9.394195e-17    0.000000   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00    1.002413   \n",
       "min   -1.560974e+00 -1.448751e+00 -1.589952e+00 -1.769499e+00   -1.898502   \n",
       "25%   -7.849821e-01 -7.988564e-01 -8.058388e-01 -7.993884e-01   -0.810706   \n",
       "50%   -3.179220e-01 -4.089954e-01 -3.220326e-01 -2.714467e-01   -0.078416   \n",
       "75%    6.774890e-01  9.254848e-01  8.690373e-01  8.804084e-01    0.939920   \n",
       "max    2.672727e+00  2.220238e+00  2.099203e+00  1.924052e+00    1.667629   \n",
       "\n",
       "                 20            21            22            23            24  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   4.440892e-16 -1.195625e-16 -3.074464e-16  3.416071e-17 -7.771561e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -2.168994e+00 -2.359782e+00 -2.366740e+00 -2.719680e+00 -2.666087e+00   \n",
       "25%   -8.139075e-01 -8.514605e-01 -7.883456e-01 -5.530684e-01 -6.123656e-01   \n",
       "50%    3.359247e-02  1.591469e-01  2.112606e-01  1.083491e-01  1.869404e-01   \n",
       "75%    8.083857e-01  8.136570e-01  8.077787e-01  8.364219e-01  8.115900e-01   \n",
       "max    1.519998e+00  1.471889e+00  1.414514e+00  1.372284e+00  1.328397e+00   \n",
       "\n",
       "                 25            26            27            28            29  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   5.636517e-16 -2.647455e-16 -8.540177e-17  1.281027e-16  1.537232e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -2.568134e+00 -2.668895e+00 -2.813072e+00 -2.618893e+00 -2.359606e+00   \n",
       "25%   -6.578782e-01 -6.947312e-01 -6.730211e-01 -7.442437e-01 -7.698181e-01   \n",
       "50%    2.308561e-01  1.772798e-01  1.600721e-01  1.615793e-01  1.190734e-01   \n",
       "75%    8.194722e-01  8.770920e-01  8.716615e-01  8.764117e-01  7.004289e-01   \n",
       "max    1.268223e+00  1.215369e+00  1.293121e+00  1.493402e+00  1.902987e+00   \n",
       "\n",
       "                 30            31            32            33            34  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   2.562053e-17  1.494531e-16 -5.978124e-17  1.708035e-17  1.024821e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -2.137348e+00 -1.873982e+00 -1.793647e+00 -1.656081e+00 -1.432336e+00   \n",
       "25%   -7.444604e-01 -7.410570e-01 -7.734584e-01 -8.048124e-01 -8.247161e-01   \n",
       "50%   -6.616850e-02 -4.437862e-02 -1.262995e-01 -2.262096e-01 -3.087757e-01   \n",
       "75%    6.439769e-01  6.640521e-01  6.742455e-01  8.361697e-01  7.766817e-01   \n",
       "max    2.160531e+00  2.310789e+00  2.828812e+00  2.433911e+00  2.349744e+00   \n",
       "\n",
       "                 35            36            37            38            39  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   1.195625e-16  2.732857e-16 -1.216975e-16  5.978124e-17  2.732857e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.430241e+00 -1.373417e+00 -1.418414e+00 -1.453707e+00 -1.680436e+00   \n",
       "25%   -8.748025e-01 -8.511364e-01 -7.784131e-01 -7.644914e-01 -6.999700e-01   \n",
       "50%   -2.417501e-01 -2.402772e-01 -1.268809e-01 -2.129934e-01 -1.860318e-01   \n",
       "75%    6.515635e-01  6.480175e-01  4.748773e-01  5.493604e-01  6.348106e-01   \n",
       "max    2.334674e+00  2.448005e+00  3.108070e+00  3.322838e+00  3.470167e+00   \n",
       "\n",
       "                 40            41            42            43            44  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean  -1.494531e-16 -1.024821e-16  8.540177e-17  3.074464e-16 -3.416071e-17   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.483614e+00 -1.620067e+00 -1.778046e+00 -1.609948e+00 -1.303898e+00   \n",
       "25%   -7.390302e-01 -7.093139e-01 -6.587522e-01 -6.557863e-01 -6.793257e-01   \n",
       "50%   -1.742944e-01 -1.972008e-01 -1.730277e-01 -2.735576e-01 -3.254731e-01   \n",
       "75%    5.757088e-01  6.294876e-01  5.624103e-01  4.337440e-01  2.268742e-01   \n",
       "max    3.574989e+00  3.245601e+00  3.798951e+00  4.227453e+00  3.346264e+00   \n",
       "\n",
       "                 45            46            47            48            49  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   1.708035e-16  1.366428e-16  2.391250e-16 -1.366428e-16 -3.074464e-16   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.202190e+00 -1.411667e+00 -1.468270e+00 -1.447799e+00 -1.498229e+00   \n",
       "25%   -6.891506e-01 -6.709773e-01 -7.435627e-01 -7.131495e-01 -6.509652e-01   \n",
       "50%   -2.939871e-01 -2.398208e-01 -2.139841e-01 -2.015434e-01 -1.851537e-01   \n",
       "75%    2.974485e-01  3.685825e-01  4.605360e-01  4.627081e-01  3.558478e-01   \n",
       "max    4.255258e+00  4.954231e+00  3.894165e+00  4.075316e+00  4.553653e+00   \n",
       "\n",
       "                 50            51            52            53            54  \\\n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02   \n",
       "mean   3.416071e-17  1.024821e-16  3.416071e-17 -1.451830e-16  2.775558e-17   \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00   \n",
       "min   -1.341343e+00 -1.313126e+00 -1.449472e+00 -1.364897e+00 -1.229092e+00   \n",
       "25%   -6.380641e-01 -6.394049e-01 -7.999231e-01 -7.642025e-01 -7.270112e-01   \n",
       "50%   -1.810370e-01 -2.102002e-01 -1.645716e-01 -2.252935e-01 -2.532164e-01   \n",
       "75%    3.970293e-01  3.438640e-01  5.950106e-01  4.886751e-01  3.973675e-01   \n",
       "max    7.039574e+00  5.980752e+00  4.016680e+00  3.330819e+00  5.008027e+00   \n",
       "\n",
       "                 55            56            57            58            59  \n",
       "count  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  2.080000e+02  \n",
       "mean  -2.391250e-16  3.416071e-17 -1.110223e-16  1.345078e-16  7.686159e-17  \n",
       "std    1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  1.002413e+00  \n",
       "min   -1.366868e+00 -1.302971e+00 -1.185113e+00 -1.271603e+00 -1.176985e+00  \n",
       "25%   -6.678488e-01 -7.138771e-01 -6.738235e-01 -6.918580e-01 -6.788714e-01  \n",
       "50%   -2.396997e-01 -3.240352e-01 -3.329639e-01 -2.499546e-01 -2.405314e-01  \n",
       "75%    4.112618e-01  4.513169e-01  3.719959e-01  3.865486e-01  4.020352e-01  \n",
       "max    5.448568e+00  4.795888e+00  5.585599e+00  4.615037e+00  7.450343e+00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = sonar_data.iloc[:,:60]\n",
    "y = sonar_data.iloc[:,60]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "\n",
    "X_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fcb71",
   "metadata": {
    "papermill": {
     "duration": 0.01272,
     "end_time": "2023-05-29T04:19:47.040410",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.027690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each feature is now normalized with its own z-score, which represents how far away each data point is (in terms of standard deviations) from the mean. Each is centered on 0, where negative numbers represent points below the mean and positive numbers represent points above. The table above shows this, so we are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cecaa7",
   "metadata": {
    "papermill": {
     "duration": 0.012815,
     "end_time": "2023-05-29T04:19:47.066217",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.053402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Prepare the Training and Test Data Sets\n",
    "\n",
    "We will use `train_test_split` from sklearn to split our data into training and testing sets. It splits our data into four different variables, which are described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a16af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.095010Z",
     "iopub.status.busy": "2023-05-29T04:19:47.094598Z",
     "iopub.status.idle": "2023-05-29T04:19:47.167195Z",
     "shell.execute_reply": "2023-05-29T04:19:47.166135Z"
    },
    "papermill": {
     "duration": 0.090778,
     "end_time": "2023-05-29T04:19:47.170118",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.079340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fceda9",
   "metadata": {
    "papermill": {
     "duration": 0.012666,
     "end_time": "2023-05-29T04:19:47.195960",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.183294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, the data is split into features (labeled \"X\") for training and testing, and labels (labeled \"y\") for training and testing. I chose to hold back 20% of the data for testing, because that is a common value to use for the hold-out method. I also set the random state to 0, so my results are reproducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5141da",
   "metadata": {
    "papermill": {
     "duration": 0.012894,
     "end_time": "2023-05-29T04:19:47.221806",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.208912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Instantiate and Configure an SVM\n",
    "\n",
    "Here we start by importing the `SVC` class from sklearn, and setting some hyperparameters. We start with the default settings, where C is the regularization parameter, the kernel used is rbf (radial basis function), gamma is the kernel coefficient used ('scale' calculates it as 1 / (number of features * feature variance)), tol is the stopping criteria for when the weights are not changing by much anymore (indication that a minimum has been reaching in the loss function), class_weight is an option to set weights for the loss function for different classes (balanced will create weights inversely proportional to their frequencies), max_iter is the max number of iterations through the training set it will use before stopping (-1 sets it to no limit), and random_state allows you to save the random number generation used for shuffling the data so results are reproducible (None will make the shuffle random each time, any integer will represent a random pattern). There are other parameters that could be set, but in class these parameters have been shown to be the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840213e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.249678Z",
     "iopub.status.busy": "2023-05-29T04:19:47.249266Z",
     "iopub.status.idle": "2023-05-29T04:19:47.339783Z",
     "shell.execute_reply": "2023-05-29T04:19:47.338361Z"
    },
    "papermill": {
     "duration": 0.108322,
     "end_time": "2023-05-29T04:19:47.343059",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.234737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(C=1.0, kernel='rbf', \n",
    "                gamma='scale', \n",
    "                tol=0.001, class_weight='balanced', \n",
    "                max_iter=-1, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c781be",
   "metadata": {
    "papermill": {
     "duration": 0.012586,
     "end_time": "2023-05-29T04:19:47.368880",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.356294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have now initiated our SVM model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf81c86",
   "metadata": {
    "papermill": {
     "duration": 0.012683,
     "end_time": "2023-05-29T04:19:47.394597",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.381914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Train the SVM\n",
    "\n",
    "Training the model is easy with this implementation. All we need to do is call the `fit` method and provide the training features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde651c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.422901Z",
     "iopub.status.busy": "2023-05-29T04:19:47.421811Z",
     "iopub.status.idle": "2023-05-29T04:19:47.442557Z",
     "shell.execute_reply": "2023-05-29T04:19:47.441449Z"
    },
    "papermill": {
     "duration": 0.037553,
     "end_time": "2023-05-29T04:19:47.444985",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.407432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced', random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57aee1",
   "metadata": {
    "papermill": {
     "duration": 0.013043,
     "end_time": "2023-05-29T04:19:47.471213",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.458170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With this run, we are now ready to use our trained model and examine the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0c3bf",
   "metadata": {
    "papermill": {
     "duration": 0.012756,
     "end_time": "2023-05-29T04:19:47.497070",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.484314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Validate and Test the SVM\n",
    "\n",
    "We will call on the `score` method and use both the training data and the testing data. This will give us the accuracy the model achieved while training, and how well it does on data it has never seen before in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ec56c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.525693Z",
     "iopub.status.busy": "2023-05-29T04:19:47.524893Z",
     "iopub.status.idle": "2023-05-29T04:19:47.541221Z",
     "shell.execute_reply": "2023-05-29T04:19:47.539678Z"
    },
    "papermill": {
     "duration": 0.033458,
     "end_time": "2023-05-29T04:19:47.543629",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.510171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9939759036144579\n",
      "Testing accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \" + str(svm_model.score(X_train,y_train)))\n",
    "print(\"Testing accuracy: \" + str(svm_model.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be22c16",
   "metadata": {
    "papermill": {
     "duration": 0.012954,
     "end_time": "2023-05-29T04:19:47.570240",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.557286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have a very high training accuracy score (99.4%), but a low testing score (83.3%). This may be an indication of over-fitting in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f6ac",
   "metadata": {
    "papermill": {
     "duration": 0.012889,
     "end_time": "2023-05-29T04:19:47.596223",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.583334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 8: Demonstrate Making Predictions\n",
    "\n",
    "It is time to show what the trained model can do. I created a random array of data that has 60 values, all between 0 and 1, just like the true features. By plugging these features into the model and using the `predict` method, it will print a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075fa245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.624615Z",
     "iopub.status.busy": "2023-05-29T04:19:47.623926Z",
     "iopub.status.idle": "2023-05-29T04:19:47.631721Z",
     "shell.execute_reply": "2023-05-29T04:19:47.630276Z"
    },
    "papermill": {
     "duration": 0.024651,
     "end_time": "2023-05-29T04:19:47.634011",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.609360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_features = pd.DataFrame([random.uniform(0, 1) for i in range(60)]).values.reshape(1,-1)\n",
    "#random_features = sonar_data.iloc[24,:60].values.reshape(1,-1)\n",
    "\n",
    "prediction = svm_model.predict(random_features)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65906e21",
   "metadata": {
    "papermill": {
     "duration": 0.013176,
     "end_time": "2023-05-29T04:19:47.660440",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.647264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here it predicts \"M\", which means a mine. I also commented out a line that takes the 24th row of the original dataframe and plugs that back into the model. This was just a test to see a different result. If you uncomment it and run the cell, the model will predict an \"R\", which means rock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b92323",
   "metadata": {
    "papermill": {
     "duration": 0.018628,
     "end_time": "2023-05-29T04:19:47.695611",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.676983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 9: Evaluate (and Improve?)\n",
    "\n",
    "The current testing accuracy is 83.3%. This model is currently underperforming and is not a good classifier. It performed worse than trained humans, who had accuracies from 88% to 97%. We have a high training accuracy, and a low testing accuracy, which indicates overfitting is likely.\n",
    "\n",
    "There are 60 features, which can lead to the perceptron trying to fit the noise instead of the pattern. A dimension reduction technique like PCA could be useful. I think decreasing the regularization parameter \"C\" would help the model not overfit the data, but I will try increasing it as well. This parameter is used to introduce penalties to the learning algorithm, so it doesn't become overly complicated in exchange for less accurate training results. Experimenting with different kernels such as poly or linear might also yield a better decision boundary. It all depends on the data, so we must experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b76ec5",
   "metadata": {
    "papermill": {
     "duration": 0.012974,
     "end_time": "2023-05-29T04:19:47.721812",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.708838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take another crack at the model. I adjusted each of the hyperparameters mentioned above, and this combination provides the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c83e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T04:19:47.751051Z",
     "iopub.status.busy": "2023-05-29T04:19:47.750215Z",
     "iopub.status.idle": "2023-05-29T04:19:47.773086Z",
     "shell.execute_reply": "2023-05-29T04:19:47.771697Z"
    },
    "papermill": {
     "duration": 0.040665,
     "end_time": "2023-05-29T04:19:47.775688",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.735023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Testing accuracy: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(C=4.2, kernel='rbf', \n",
    "                gamma='scale', degree=3,\n",
    "                tol=0.001, class_weight='balanced', \n",
    "                max_iter=-1, random_state=0)\n",
    "\n",
    "svm_model.fit(X_train,y_train)\n",
    "print(\"Training accuracy: \" + str(svm_model.score(X_train,y_train)))\n",
    "print(\"Testing accuracy: \" + str(svm_model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787ea4f7",
   "metadata": {
    "papermill": {
     "duration": 0.013554,
     "end_time": "2023-05-29T04:19:47.803298",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.789744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have achieved 92.9% accuracy on our testing set. That is great! Our model now performs right in the middle of the average human accuracy (88% to 97%). I increased the regularization parameter \"C\" to 4.2, which allows the model to get a bit more complicated. Using the poly kernel, I was able to get about 90.5% accuracy, but ultimately went back to the rbf kernel. No other kernels performed as well. I played around with the other settings, but ultimately the default for each was the best.\n",
    "\n",
    "The training accuracy is 100%, which worries me. I was able to get both accuracies around the 88% mark with one set of hyperparameters, but because I could not get better than that I changed back. Our model may still be overfitting to this data set. If I were to continue this experiment, I would do k-fold cross validation with the data to make sure the accuracies the model performs at are seen even when training and testing on different sets. I also think I would use a dimension reduction technique like PCA. I will not experiment with that here because it is beyond the scope of the workbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78125a59",
   "metadata": {
    "papermill": {
     "duration": 0.013655,
     "end_time": "2023-05-29T04:19:47.830784",
     "exception": false,
     "start_time": "2023-05-29T04:19:47.817129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "In this workbook I explored the sonar dataset again, used the sklearn SVM to train and test a model that predicts whether readings indicate solid rock or a mine, and tried to improve the model accuracy by tuning hyperparameters. I learned how the standard scaler function works from sklearn. I wonder if different scaling techniques influence the performance of the model? Before we did min and max scaling between 0 and 1. I was able to understand that my model was overfitting the data by observing the difference between the training and testing accuracies. I thought I could improve the results by decreasing the regularization parameter \"C\", to force the model to become simpler. This did improve the testing accuracy up to about 88%, but I was surprised that when I increased the regularization parameter (allowing the model to become a lot more complicated) the testing accuracy increased to 93%. I did not expect that result. I would love to test the model on some new validation data.\n",
    "\n",
    "The support vector machine did achieve a greater accuracy than the perceptron I used in notebook 4 (85.7%). This is promising, but I still think with more exploration and time to explore the options I have stated above, the accuracy could increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.335082,
   "end_time": "2023-05-29T04:19:48.567644",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-29T04:19:32.232562",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
