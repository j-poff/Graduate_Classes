{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec797ac2",
   "metadata": {
    "papermill": {
     "duration": 0.011389,
     "end_time": "2023-04-30T19:56:27.461875",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.450486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains two parts. **Part 1, Testing Perceptrons**, provides you an opportunity to demonstrate your ability to apply course concepts by implementing a test function for a perceptron. **Part 2, Mine Detection**, provides you an opportunity to practice using widely-used ML libraries and an ML workflow to solve a classification problem.\n",
    "\n",
    "You do not need to complete Part 1 in order to complete Part 2. If you get stuck on Part 1, and choose to work on Part 2, be sure that all of your code for Part 1 runs without error. You can comment out your code in Part 1 if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef875171",
   "metadata": {
    "papermill": {
     "duration": 0.009931,
     "end_time": "2023-04-30T19:56:27.482025",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.472094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Testing Perceptrons\n",
    "\n",
    "Given a simple Perceptron classifier, and a test set cancer diagnoses, demonstrate your ability to implement a perceptron's `test` function, such that it returns the accuracy of its predictions for the class labels of samples in a training set.\n",
    "\n",
    "## The Perceptron Implementation\n",
    "\n",
    "Let's first introduce the classifier, which you should find familiar, and you do not need to modify. Notice that the `test` method is stubbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e36ae2",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:27.504248Z",
     "iopub.status.busy": "2023-04-30T19:56:27.503817Z",
     "iopub.status.idle": "2023-04-30T19:56:27.522296Z",
     "shell.execute_reply": "2023-04-30T19:56:27.521103Z"
    },
    "papermill": {
     "duration": 0.033406,
     "end_time": "2023-04-30T19:56:27.525466",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.492060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, alpha = 0.1, max_epochs = 1000):\n",
    "        self.alpha = alpha\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def train(self, training_set):\n",
    "        self.weights = self._initial_weights(len(training_set[0]))\n",
    "        for i in range(self.max_epochs):\n",
    "            correct_classifications = 0\n",
    "            for record in training_set:\n",
    "                y = record[0]\n",
    "                y_hat = self.predict(record[1:])\n",
    "                if y == y_hat: correct_classifications += 1\n",
    "                self._update_weights(y - y_hat, [1] + record[1:])\n",
    "            if correct_classifications / len(training_set) == 1.0:\n",
    "                print(f\"Epoch {i} Accuracy: {correct_classifications / len(training_set)}\")\n",
    "                break\n",
    "\n",
    "    def predict(self, features):\n",
    "        return self._sign_of(self._dot_product(self.weights, [1] + features))\n",
    "\n",
    "    def test(self, test_set):\n",
    "        pass\n",
    "\n",
    "    def _update_weights(self, error, features):\n",
    "        self.weights[0] += self.alpha * error\n",
    "        for i in range(1, len(self.weights)):\n",
    "            self.weights[i] += self.alpha * error * features[i]\n",
    "\n",
    "    def _dot_product(self, w, x):\n",
    "        return sum([w * x for w, x in zip(w, x)])\n",
    "\n",
    "    def _sign_of(self, value):\n",
    "        return 1 if value >= 0 else -1\n",
    "\n",
    "    def _initial_weights(self, length):\n",
    "        return [random.uniform(0, 1) for _ in range(length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96c922",
   "metadata": {
    "papermill": {
     "duration": 0.011575,
     "end_time": "2023-04-30T19:56:27.547118",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.535543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The Data Set\n",
    "\n",
    "There is no need for you to manually load the data set. We have provided a subset of the cancer diagnoses data set here, already split into a training set, `diagnoses_training`, and a test set, `diagnoses_test`. Each data set is a simple, two-dimensional Python list, where each sub-list represents the attributes for one diagnosis.\n",
    "\n",
    "We have preprocessed the data, such that the **first dimension is the class label**: a `1` indicating malignant, and `-1` indicating benign.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b80a82",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:27.569434Z",
     "iopub.status.busy": "2023-04-30T19:56:27.569019Z",
     "iopub.status.idle": "2023-04-30T19:56:27.642131Z",
     "shell.execute_reply": "2023-04-30T19:56:27.640969Z"
    },
    "papermill": {
     "duration": 0.088116,
     "end_time": "2023-04-30T19:56:27.645267",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.557151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diagnoses_training = [\n",
    "    [1, 17.99, 10.38, 122.8, 1001, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193, 25.38, 17.33, 184.6, 2019, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189],\n",
    "    [1, 20.57, 17.77, 132.9, 1326, 0.08474, 0.07864, 0.0869, 0.07017, 0.1812, 0.05667, 0.5435, 0.7339, 3.398, 74.08, 0.005225, 0.01308, 0.0186, 0.0134, 0.01389, 0.003532, 24.99, 23.41, 158.8, 1956, 0.1238, 0.1866, 0.2416, 0.186, 0.275, 0.08902],\n",
    "    [1, 19.69, 21.25, 130, 1203, 0.1096, 0.1599, 0.1974, 0.1279, 0.2069, 0.05999, 0.7456, 0.7869, 4.585, 94.03, 0.00615, 0.04006, 0.03832, 0.02058, 0.0225, 0.004571, 23.57, 25.53, 152.5, 1709, 0.1444, 0.4245, 0.4504, 0.243, 0.3613, 0.08758],\n",
    "    [1, 11.42, 20.38, 77.58, 386.1, 0.1425, 0.2839, 0.2414, 0.1052, 0.2597, 0.09744, 0.4956, 1.156, 3.445, 27.23, 0.00911, 0.07458, 0.05661, 0.01867, 0.05963, 0.009208, 14.91, 26.5, 98.87, 567.7, 0.2098, 0.8663, 0.6869, 0.2575, 0.6638, 0.173],\n",
    "    [1, 20.29, 14.34, 135.1, 1297, 0.1003, 0.1328, 0.198, 0.1043, 0.1809, 0.05883, 0.7572, 0.7813, 5.438, 94.44, 0.01149, 0.02461, 0.05688, 0.01885, 0.01756, 0.005115, 22.54, 16.67, 152.2, 1575, 0.1374, 0.205, 0.4, 0.1625, 0.2364, 0.07678],\n",
    "    [1, 12.45, 15.7, 82.57, 477.1, 0.1278, 0.17, 0.1578, 0.08089, 0.2087, 0.07613, 0.3345, 0.8902, 2.217, 27.19, 0.00751, 0.03345, 0.03672, 0.01137, 0.02165, 0.005082, 15.47, 23.75, 103.4, 741.6, 0.1791, 0.5249, 0.5355, 0.1741, 0.3985, 0.1244],\n",
    "    [1, 18.25, 19.98, 119.6, 1040, 0.09463, 0.109, 0.1127, 0.074, 0.1794, 0.05742, 0.4467, 0.7732, 3.18, 53.91, 0.004314, 0.01382, 0.02254, 0.01039, 0.01369, 0.002179, 22.88, 27.66, 153.2, 1606, 0.1442, 0.2576, 0.3784, 0.1932, 0.3063, 0.08368],\n",
    "    [1, 13.71, 20.83, 90.2, 577.9, 0.1189, 0.1645, 0.09366, 0.05985, 0.2196, 0.07451, 0.5835, 1.377, 3.856, 50.96, 0.008805, 0.03029, 0.02488, 0.01448, 0.01486, 0.005412, 17.06, 28.14, 110.6, 897, 0.1654, 0.3682, 0.2678, 0.1556, 0.3196, 0.1151],\n",
    "    [1, 13, 21.82, 87.5, 519.8, 0.1273, 0.1932, 0.1859, 0.09353, 0.235, 0.07389, 0.3063, 1.002, 2.406, 24.32, 0.005731, 0.03502, 0.03553, 0.01226, 0.02143, 0.003749, 15.49, 30.73, 106.2, 739.3, 0.1703, 0.5401, 0.539, 0.206, 0.4378, 0.1072],\n",
    "    [1, 12.46, 24.04, 83.97, 475.9, 0.1186, 0.2396, 0.2273, 0.08543, 0.203, 0.08243, 0.2976, 1.599, 2.039, 23.94, 0.007149, 0.07217, 0.07743, 0.01432, 0.01789, 0.01008, 15.09, 40.68, 97.65, 711.4, 0.1853, 1.058, 1.105, 0.221, 0.4366, 0.2075],\n",
    "    [1, 16.02, 23.24, 102.7, 797.8, 0.08206, 0.06669, 0.03299, 0.03323, 0.1528, 0.05697, 0.3795, 1.187, 2.466, 40.51, 0.004029, 0.009269, 0.01101, 0.007591, 0.0146, 0.003042, 19.19, 33.88, 123.8, 1150, 0.1181, 0.1551, 0.1459, 0.09975, 0.2948, 0.08452],\n",
    "    [1, 15.78, 17.89, 103.6, 781, 0.0971, 0.1292, 0.09954, 0.06606, 0.1842, 0.06082, 0.5058, 0.9849, 3.564, 54.16, 0.005771, 0.04061, 0.02791, 0.01282, 0.02008, 0.004144, 20.42, 27.28, 136.5, 1299, 0.1396, 0.5609, 0.3965, 0.181, 0.3792, 0.1048],\n",
    "    [1, 19.17, 24.8, 132.4, 1123, 0.0974, 0.2458, 0.2065, 0.1118, 0.2397, 0.078, 0.9555, 3.568, 11.07, 116.2, 0.003139, 0.08297, 0.0889, 0.0409, 0.04484, 0.01284, 20.96, 29.94, 151.7, 1332, 0.1037, 0.3903, 0.3639, 0.1767, 0.3176, 0.1023],\n",
    "    [1, 15.85, 23.95, 103.7, 782.7, 0.08401, 0.1002, 0.09938, 0.05364, 0.1847, 0.05338, 0.4033, 1.078, 2.903, 36.58, 0.009769, 0.03126, 0.05051, 0.01992, 0.02981, 0.003002, 16.84, 27.66, 112, 876.5, 0.1131, 0.1924, 0.2322, 0.1119, 0.2809, 0.06287],\n",
    "    [1, 13.73, 22.61, 93.6, 578.3, 0.1131, 0.2293, 0.2128, 0.08025, 0.2069, 0.07682, 0.2121, 1.169, 2.061, 19.21, 0.006429, 0.05936, 0.05501, 0.01628, 0.01961, 0.008093, 15.03, 32.01, 108.8, 697.7, 0.1651, 0.7725, 0.6943, 0.2208, 0.3596, 0.1431],\n",
    "    [1, 14.54, 27.54, 96.73, 658.8, 0.1139, 0.1595, 0.1639, 0.07364, 0.2303, 0.07077, 0.37, 1.033, 2.879, 32.55, 0.005607, 0.0424, 0.04741, 0.0109, 0.01857, 0.005466, 17.46, 37.13, 124.1, 943.2, 0.1678, 0.6577, 0.7026, 0.1712, 0.4218, 0.1341],\n",
    "    [1, 14.68, 20.13, 94.74, 684.5, 0.09867, 0.072, 0.07395, 0.05259, 0.1586, 0.05922, 0.4727, 1.24, 3.195, 45.4, 0.005718, 0.01162, 0.01998, 0.01109, 0.0141, 0.002085, 19.07, 30.88, 123.4, 1138, 0.1464, 0.1871, 0.2914, 0.1609, 0.3029, 0.08216],\n",
    "    [1, 16.13, 20.68, 108.1, 798.8, 0.117, 0.2022, 0.1722, 0.1028, 0.2164, 0.07356, 0.5692, 1.073, 3.854, 54.18, 0.007026, 0.02501, 0.03188, 0.01297, 0.01689, 0.004142, 20.96, 31.48, 136.8, 1315, 0.1789, 0.4233, 0.4784, 0.2073, 0.3706, 0.1142],\n",
    "    [1, 19.81, 22.15, 130, 1260, 0.09831, 0.1027, 0.1479, 0.09498, 0.1582, 0.05395, 0.7582, 1.017, 5.865, 112.4, 0.006494, 0.01893, 0.03391, 0.01521, 0.01356, 0.001997, 27.32, 30.88, 186.8, 2398, 0.1512, 0.315, 0.5372, 0.2388, 0.2768, 0.07615],\n",
    "    [-1, 13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766, 0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023, 15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259],\n",
    "    [-1, 13.08, 15.71, 85.63, 520, 0.1075, 0.127, 0.04568, 0.0311, 0.1967, 0.06811, 0.1852, 0.7477, 1.383, 14.67, 0.004097, 0.01898, 0.01698, 0.00649, 0.01678, 0.002425, 14.5, 20.49, 96.09, 630.5, 0.1312, 0.2776, 0.189, 0.07283, 0.3184, 0.08183],\n",
    "    [-1, 9.504, 12.44, 60.34, 273.9, 0.1024, 0.06492, 0.02956, 0.02076, 0.1815, 0.06905, 0.2773, 0.9768, 1.909, 15.7, 0.009606, 0.01432, 0.01985, 0.01421, 0.02027, 0.002968, 10.23, 15.66, 65.13, 314.9, 0.1324, 0.1148, 0.08867, 0.06227, 0.245, 0.07773],\n",
    "    [1, 15.34, 14.26, 102.5, 704.4, 0.1073, 0.2135, 0.2077, 0.09756, 0.2521, 0.07032, 0.4388, 0.7096, 3.384, 44.91, 0.006789, 0.05328, 0.06446, 0.02252, 0.03672, 0.004394, 18.07, 19.08, 125.1, 980.9, 0.139, 0.5954, 0.6305, 0.2393, 0.4667, 0.09946],\n",
    "    [1, 21.16, 23.04, 137.2, 1404, 0.09428, 0.1022, 0.1097, 0.08632, 0.1769, 0.05278, 0.6917, 1.127, 4.303, 93.99, 0.004728, 0.01259, 0.01715, 0.01038, 0.01083, 0.001987, 29.17, 35.59, 188, 2615, 0.1401, 0.26, 0.3155, 0.2009, 0.2822, 0.07526],\n",
    "    [1, 16.65, 21.38, 110, 904.6, 0.1121, 0.1457, 0.1525, 0.0917, 0.1995, 0.0633, 0.8068, 0.9017, 5.455, 102.6, 0.006048, 0.01882, 0.02741, 0.0113, 0.01468, 0.002801, 26.46, 31.56, 177, 2215, 0.1805, 0.3578, 0.4695, 0.2095, 0.3613, 0.09564],\n",
    "    [1, 17.14, 16.4, 116, 912.7, 0.1186, 0.2276, 0.2229, 0.1401, 0.304, 0.07413, 1.046, 0.976, 7.276, 111.4, 0.008029, 0.03799, 0.03732, 0.02397, 0.02308, 0.007444, 22.25, 21.4, 152.4, 1461, 0.1545, 0.3949, 0.3853, 0.255, 0.4066, 0.1059],\n",
    "    [1, 14.58, 21.53, 97.41, 644.8, 0.1054, 0.1868, 0.1425, 0.08783, 0.2252, 0.06924, 0.2545, 0.9832, 2.11, 21.05, 0.004452, 0.03055, 0.02681, 0.01352, 0.01454, 0.003711, 17.62, 33.21, 122.4, 896.9, 0.1525, 0.6643, 0.5539, 0.2701, 0.4264, 0.1275],\n",
    "    [1, 18.61, 20.25, 122.1, 1094, 0.0944, 0.1066, 0.149, 0.07731, 0.1697, 0.05699, 0.8529, 1.849, 5.632, 93.54, 0.01075, 0.02722, 0.05081, 0.01911, 0.02293, 0.004217, 21.31, 27.26, 139.9, 1403, 0.1338, 0.2117, 0.3446, 0.149, 0.2341, 0.07421],\n",
    "    [1, 15.3, 25.27, 102.4, 732.4, 0.1082, 0.1697, 0.1683, 0.08751, 0.1926, 0.0654, 0.439, 1.012, 3.498, 43.5, 0.005233, 0.03057, 0.03576, 0.01083, 0.01768, 0.002967, 20.27, 36.71, 149.3, 1269, 0.1641, 0.611, 0.6335, 0.2024, 0.4027, 0.09876],\n",
    "    [1, 17.57, 15.05, 115, 955.1, 0.09847, 0.1157, 0.09875, 0.07953, 0.1739, 0.06149, 0.6003, 0.8225, 4.655, 61.1, 0.005627, 0.03033, 0.03407, 0.01354, 0.01925, 0.003742, 20.01, 19.52, 134.9, 1227, 0.1255, 0.2812, 0.2489, 0.1456, 0.2756, 0.07919],\n",
    "    [1, 18.63, 25.11, 124.8, 1088, 0.1064, 0.1887, 0.2319, 0.1244, 0.2183, 0.06197, 0.8307, 1.466, 5.574, 105, 0.006248, 0.03374, 0.05196, 0.01158, 0.02007, 0.00456, 23.15, 34.01, 160.5, 1670, 0.1491, 0.4257, 0.6133, 0.1848, 0.3444, 0.09782],\n",
    "    [1, 11.84, 18.7, 77.93, 440.6, 0.1109, 0.1516, 0.1218, 0.05182, 0.2301, 0.07799, 0.4825, 1.03, 3.475, 41, 0.005551, 0.03414, 0.04205, 0.01044, 0.02273, 0.005667, 16.82, 28.12, 119.4, 888.7, 0.1637, 0.5775, 0.6956, 0.1546, 0.4761, 0.1402],\n",
    "    [1, 17.02, 23.98, 112.8, 899.3, 0.1197, 0.1496, 0.2417, 0.1203, 0.2248, 0.06382, 0.6009, 1.398, 3.999, 67.78, 0.008268, 0.03082, 0.05042, 0.01112, 0.02102, 0.003854, 20.88, 32.09, 136.1, 1344, 0.1634, 0.3559, 0.5588, 0.1847, 0.353, 0.08482],\n",
    "    [1, 19.27, 26.47, 127.9, 1162, 0.09401, 0.1719, 0.1657, 0.07593, 0.1853, 0.06261, 0.5558, 0.6062, 3.528, 68.17, 0.005015, 0.03318, 0.03497, 0.009643, 0.01543, 0.003896, 24.15, 30.9, 161.4, 1813, 0.1509, 0.659, 0.6091, 0.1785, 0.3672, 0.1123],\n",
    "    [1, 16.13, 17.88, 107, 807.2, 0.104, 0.1559, 0.1354, 0.07752, 0.1998, 0.06515, 0.334, 0.6857, 2.183, 35.03, 0.004185, 0.02868, 0.02664, 0.009067, 0.01703, 0.003817, 20.21, 27.26, 132.7, 1261, 0.1446, 0.5804, 0.5274, 0.1864, 0.427, 0.1233],\n",
    "    [1, 16.74, 21.59, 110.1, 869.5, 0.0961, 0.1336, 0.1348, 0.06018, 0.1896, 0.05656, 0.4615, 0.9197, 3.008, 45.19, 0.005776, 0.02499, 0.03695, 0.01195, 0.02789, 0.002665, 20.01, 29.02, 133.5, 1229, 0.1563, 0.3835, 0.5409, 0.1813, 0.4863, 0.08633],\n",
    "    [1, 14.25, 21.72, 93.63, 633, 0.09823, 0.1098, 0.1319, 0.05598, 0.1885, 0.06125, 0.286, 1.019, 2.657, 24.91, 0.005878, 0.02995, 0.04815, 0.01161, 0.02028, 0.004022, 15.89, 30.36, 116.2, 799.6, 0.1446, 0.4238, 0.5186, 0.1447, 0.3591, 0.1014],\n",
    "    [-1, 13.03, 18.42, 82.61, 523.8, 0.08983, 0.03766, 0.02562, 0.02923, 0.1467, 0.05863, 0.1839, 2.342, 1.17, 14.16, 0.004352, 0.004899, 0.01343, 0.01164, 0.02671, 0.001777, 13.3, 22.81, 84.46, 545.9, 0.09701, 0.04619, 0.04833, 0.05013, 0.1987, 0.06169],\n",
    "    [1, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504, 1.214, 2.188, 8.077, 106, 0.006883, 0.01094, 0.01818, 0.01917, 0.007882, 0.001754, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504],\n",
    "    [1, 13.48, 20.82, 88.4, 559.2, 0.1016, 0.1255, 0.1063, 0.05439, 0.172, 0.06419, 0.213, 0.5914, 1.545, 18.52, 0.005367, 0.02239, 0.03049, 0.01262, 0.01377, 0.003187, 15.53, 26.02, 107.3, 740.4, 0.161, 0.4225, 0.503, 0.2258, 0.2807, 0.1071],\n",
    "    [1, 13.44, 21.58, 86.18, 563, 0.08162, 0.06031, 0.0311, 0.02031, 0.1784, 0.05587, 0.2385, 0.8265, 1.572, 20.53, 0.00328, 0.01102, 0.0139, 0.006881, 0.0138, 0.001286, 15.93, 30.25, 102.5, 787.9, 0.1094, 0.2043, 0.2085, 0.1112, 0.2994, 0.07146],\n",
    "    [1, 10.95, 21.35, 71.9, 371.1, 0.1227, 0.1218, 0.1044, 0.05669, 0.1895, 0.0687, 0.2366, 1.428, 1.822, 16.97, 0.008064, 0.01764, 0.02595, 0.01037, 0.01357, 0.00304, 12.84, 35.34, 87.22, 514, 0.1909, 0.2698, 0.4023, 0.1424, 0.2964, 0.09606],\n",
    "    [1, 19.07, 24.81, 128.3, 1104, 0.09081, 0.219, 0.2107, 0.09961, 0.231, 0.06343, 0.9811, 1.666, 8.83, 104.9, 0.006548, 0.1006, 0.09723, 0.02638, 0.05333, 0.007646, 24.09, 33.17, 177.4, 1651, 0.1247, 0.7444, 0.7242, 0.2493, 0.467, 0.1038],\n",
    "    [1, 13.28, 20.28, 87.32, 545.2, 0.1041, 0.1436, 0.09847, 0.06158, 0.1974, 0.06782, 0.3704, 0.8249, 2.427, 31.33, 0.005072, 0.02147, 0.02185, 0.00956, 0.01719, 0.003317, 17.38, 28, 113.1, 907.2, 0.153, 0.3724, 0.3664, 0.1492, 0.3739, 0.1027],\n",
    "    [1, 13.17, 21.81, 85.42, 531.5, 0.09714, 0.1047, 0.08259, 0.05252, 0.1746, 0.06177, 0.1938, 0.6123, 1.334, 14.49, 0.00335, 0.01384, 0.01452, 0.006853, 0.01113, 0.00172, 16.23, 29.89, 105.5, 740.7, 0.1503, 0.3904, 0.3728, 0.1607, 0.3693, 0.09618],\n",
    "    [1, 18.65, 17.6, 123.7, 1076, 0.1099, 0.1686, 0.1974, 0.1009, 0.1907, 0.06049, 0.6289, 0.6633, 4.293, 71.56, 0.006294, 0.03994, 0.05554, 0.01695, 0.02428, 0.003535, 22.82, 21.32, 150.6, 1567, 0.1679, 0.509, 0.7345, 0.2378, 0.3799, 0.09185],\n",
    "    [-1, 8.196, 16.84, 51.71, 201.9, 0.086, 0.05943, 0.01588, 0.005917, 0.1769, 0.06503, 0.1563, 0.9567, 1.094, 8.205, 0.008968, 0.01646, 0.01588, 0.005917, 0.02574, 0.002582, 8.964, 21.96, 57.26, 242.2, 0.1297, 0.1357, 0.0688, 0.02564, 0.3105, 0.07409]\n",
    "]\n",
    "\n",
    "diagnoses_test = [\n",
    "    [-1, 8.95, 15.76, 58.74, 245.2, 0.09462, 0.1243, 0.09263, 0.02308, 0.1305, 0.07163, 0.3132, 0.9789, 3.28, 16.94, 0.01835, 0.0676, 0.09263, 0.02308, 0.02384, 0.005601, 9.414, 17.07, 63.34, 270, 0.1179, 0.1879, 0.1544, 0.03846, 0.1652, 0.07722],\n",
    "    [1, 15.22, 30.62, 103.4, 716.9, 0.1048, 0.2087, 0.255, 0.09429, 0.2128, 0.07152, 0.2602, 1.205, 2.362, 22.65, 0.004625, 0.04844, 0.07359, 0.01608, 0.02137, 0.006142, 17.52, 42.79, 128.7, 915, 0.1417, 0.7917, 1.17, 0.2356, 0.4089, 0.1409],\n",
    "    [-1, 11.34, 21.26, 72.48, 396.5, 0.08759, 0.06575, 0.05133, 0.01899, 0.1487, 0.06529, 0.2344, 0.9861, 1.597, 16.41, 0.009113, 0.01557, 0.02443, 0.006435, 0.01568, 0.002477, 13.01, 29.15, 83.99, 518.1, 0.1699, 0.2196, 0.312, 0.08278, 0.2829, 0.08832],\n",
    "    [1, 20.92, 25.09, 143, 1347, 0.1099, 0.2236, 0.3174, 0.1474, 0.2149, 0.06879, 0.9622, 1.026, 8.758, 118.8, 0.006399, 0.0431, 0.07845, 0.02624, 0.02057, 0.006213, 24.29, 29.41, 179.1, 1819, 0.1407, 0.4186, 0.6599, 0.2542, 0.2929, 0.09873],\n",
    "    [-1, 12.36, 18.54, 79.01, 466.7, 0.08477, 0.06815, 0.02643, 0.01921, 0.1602, 0.06066, 0.1199, 0.8944, 0.8484, 9.227, 0.003457, 0.01047, 0.01167, 0.005558, 0.01251, 0.001356, 13.29, 27.49, 85.56, 544.1, 0.1184, 0.1963, 0.1937, 0.08442, 0.2983, 0.07185],\n",
    "    [1, 21.56, 22.39, 142, 1479, 0.111, 0.1159, 0.2439, 0.1389, 0.1726, 0.05623, 1.176, 1.256, 7.673, 158.7, 0.0103, 0.02891, 0.05198, 0.02454, 0.01114, 0.004239, 25.45, 26.4, 166.1, 2027, 0.141, 0.2113, 0.4107, 0.2216, 0.206, 0.07115],\n",
    "    [-1, 9.777, 16.99, 62.5, 290.2, 0.1037, 0.08404, 0.04334, 0.01778, 0.1584, 0.07065, 0.403, 1.424, 2.747, 22.87, 0.01385, 0.02932, 0.02722, 0.01023, 0.03281, 0.004638, 11.05, 21.47, 71.68, 367, 0.1467, 0.1765, 0.13, 0.05334, 0.2533, 0.08468],\n",
    "    [1, 20.13, 28.25, 131.2, 1261, 0.0978, 0.1034, 0.144, 0.09791, 0.1752, 0.05533, 0.7655, 2.463, 5.203, 99.04, 0.005769, 0.02423, 0.0395, 0.01678, 0.01898, 0.002498, 23.69, 38.25, 155, 1731, 0.1166, 0.1922, 0.3215, 0.1628, 0.2572, 0.06637],\n",
    "    [-1, 12.63, 20.76, 82.15, 480.4, 0.09933, 0.1209, 0.1065, 0.06021, 0.1735, 0.0707, 0.3424, 1.803, 2.711, 20.48, 0.01291, 0.04042, 0.05101, 0.02295, 0.02144, 0.005891, 13.33, 25.47, 89, 527.4, 0.1287, 0.225, 0.2216, 0.1105, 0.2226, 0.08486],\n",
    "    [1, 16.6, 28.08, 108.3, 858.1, 0.08455, 0.1023, 0.09251, 0.05302, 0.159, 0.05648, 0.4564, 1.075, 3.425, 48.55, 0.005903, 0.03731, 0.0473, 0.01557, 0.01318, 0.003892, 18.98, 34.12, 126.7, 1124, 0.1139, 0.3094, 0.3403, 0.1418, 0.2218, 0.0782],\n",
    "    [-1, 14.26, 19.65, 97.83, 629.9, 0.07837, 0.2233, 0.3003, 0.07798, 0.1704, 0.07769, 0.3628, 1.49, 3.399, 29.25, 0.005298, 0.07446, 0.1435, 0.02292, 0.02566, 0.01298, 15.3, 23.73, 107, 709, 0.08949, 0.4193, 0, 0.1503, 0.07247, 0.2438, 0.08541],\n",
    "    [1, 20.6, 29.33, 140.1, 1265, 0.1178, 0.277, 0.3514, 0.152, 0.2397, 0.07016, 0.726, 1.595, 5.772, 86.22, 0.006522, 0.06158, 0.07117, 0.01664, 0.02324, 0.006185, 25.74, 39.42, 184.6, 1821, 0.165, 0.8681, 0.9387, 0.265, 0.4087, 0.124]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a388ef0",
   "metadata": {
    "papermill": {
     "duration": 0.00955,
     "end_time": "2023-04-30T19:56:27.664823",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.655273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What to Do\n",
    " \n",
    "Demonstrate your understanding and ability to have synthesized course concepts by implementing a `test` function for the perceptron. Your goal is to implement the `test` function, stubbed for you below, in the subclass `TestablePerceptron`. The `test` function should return the accuracy rate of the perceptron's prediction, given `diagnoses_test`. In the end, your work should reflect the principles seen thus far in the course.\n",
    "\n",
    "Please be sure to demonstrate:\n",
    "\n",
    "1. Your implementation, as code, in the subclass below.\n",
    "2. Using your test function, which is already done for you, following the class definition.\n",
    "\n",
    "## Tips\n",
    "\n",
    "- Be sure that you have spent time with the Exploration materials in this course.\n",
    "- Ask questions on the course forum if you get stuck (describe what you are trying to do, and errors that you encounter)\n",
    "- Keep it simple. This is quite straightforward.\n",
    "- Be sure to run the code cell containing your TestablePerceptron class and the code cell that invokes the `test` method, or use the *>> Run All* button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0b717",
   "metadata": {
    "papermill": {
     "duration": 0.009697,
     "end_time": "2023-04-30T19:56:27.685972",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.676275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Begin Your Implementation Here\n",
    "\n",
    "In the code cell below, implement the `test` function by replacing `pass` with your code. It should accept a training set as input, and *return* a number representing the accuracy of the perceptron based on predictions made with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799adc1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:27.707910Z",
     "iopub.status.busy": "2023-04-30T19:56:27.707482Z",
     "iopub.status.idle": "2023-04-30T19:56:27.715160Z",
     "shell.execute_reply": "2023-04-30T19:56:27.713688Z"
    },
    "papermill": {
     "duration": 0.022206,
     "end_time": "2023-04-30T19:56:27.718074",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.695868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestablePerceptron(Perceptron):\n",
    "    \n",
    "    def test(self, test_set):\n",
    "        correct_predictions = 0\n",
    "        for row in range(len(test_set)):\n",
    "            if test_set[row][0] == self.predict(test_set[row][1:]):\n",
    "                correct_predictions += 1\n",
    "        return  correct_predictions/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0183f",
   "metadata": {
    "papermill": {
     "duration": 0.009522,
     "end_time": "2023-04-30T19:56:27.737690",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.728168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Your TestablePerceptron should result in the following code, which trains and tests a TestablePerceptron, running without error. You do not need to modify the code below, but once your `test` method above is complete, you should see the accuracy of the test printed, instead of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd481560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:27.759433Z",
     "iopub.status.busy": "2023-04-30T19:56:27.758957Z",
     "iopub.status.idle": "2023-04-30T19:56:31.458169Z",
     "shell.execute_reply": "2023-04-30T19:56:31.456884Z"
    },
    "papermill": {
     "duration": 3.714763,
     "end_time": "2023-04-30T19:56:31.461913",
     "exception": false,
     "start_time": "2023-04-30T19:56:27.747150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5622 Accuracy: 1.0\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "perceptron = TestablePerceptron(alpha = 0.1, max_epochs = 10000)\n",
    "perceptron.train(diagnoses_training)\n",
    "print(perceptron.test(diagnoses_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8dda0a",
   "metadata": {
    "papermill": {
     "duration": 0.009828,
     "end_time": "2023-04-30T19:56:31.482589",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.472761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion of Part 1\n",
    "\n",
    "Write a few sentences here that describes what your `test` function does in \"plain English.\" Try expressing this accurately and authentically *without* giving us a line-by-line summary of what your function does (we can read the code for that). Tell us what the test error is. Then, describe how many learning epochs it took for the perceptron to achive 100% accuracy during *training*, and explain why that number changes each time you run the code cell above.\n",
    "\n",
    "My test function goes through each row of test data one by one, and passes the features to the predict function defined above. That predict function returns the label, which in this case is a -1 or 1. I compare this to the true label in the row of test data, and if it is the same I keep track of it. Then I know how many it got right compared to the total number of test data objects, which gives the testing accuracy. The accuracy is 91.7%. \n",
    "\n",
    "It took 5500 epochs to get to 100% accruacy for the perceptron. This number changes each time because the weights for the perceptrons are randomly assigned at the beginning of training. Depending on how close these weights are to the ideal trained weights, the training will take a different number of epochs to acheive 100% accruacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd242c",
   "metadata": {
    "papermill": {
     "duration": 0.009639,
     "end_time": "2023-04-30T19:56:31.502352",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.492713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: Mine Detection\n",
    "\n",
    "In this, the second, part of this notebook, you will build a classifier that can predict whether or not a sonar signature is from a mine or a rock. We'll use a version of the [sonar data set](https://www.openml.org/search?type=data&sort=runs&id=40&status=active) by Gorman and Sejnowski. Take a moment now to [familiarize yourself with the subject matter of this data set](https://datahub.io/machine-learning/sonar%23resource-sonar), and look at the details of the version of this data set, [Mines vs Rocks, hosted on Kaggle](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks).\n",
    "\n",
    "Unlike previous notebooks, where we provide code for each step of the ML process, this notebook expects each student to implement the ML workflow steps. We will get you started by providing the first step, loading the data, and providing some landmarks below. Your process should demonstrate:\n",
    "\n",
    "1. Loading the data\n",
    "2. Exploring the data\n",
    "3. Preprocessing the data\n",
    "4. Preparing the training and test sets\n",
    "5. Creating and configuring a sklearn.linear_model.Perceptron\n",
    "6. Training the perceptron\n",
    "7. Testing the perceptron\n",
    "8. Demonstrating making predictions\n",
    "9. Evaluate (and Improve) the results\n",
    "\n",
    "Can you train a classifier that can predict whether a sonar signature is from a mine or a rock? \"Three trained human subjects were each tested on 100 signals, chosen at random from the set of 208 returns used to create this data set. Their responses ranged between 88% and 97% correct.\" Can your classifier outperform the human subjects?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e68a69",
   "metadata": {
    "papermill": {
     "duration": 0.009899,
     "end_time": "2023-04-30T19:56:31.522543",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.512644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "The notebook comes pre-bundled with the [Mines vs Rocks data set](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks). Our first step is to create a pandas DataFrame from the CSV file. Note that the CSV file has no header row. Loading the CSV file into a DataFrame will make it easy for us to explore the data, preprocess it, and split it into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1178154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:31.545379Z",
     "iopub.status.busy": "2023-04-30T19:56:31.544941Z",
     "iopub.status.idle": "2023-04-30T19:56:31.569344Z",
     "shell.execute_reply": "2023-04-30T19:56:31.568313Z"
    },
    "papermill": {
     "duration": 0.03944,
     "end_time": "2023-04-30T19:56:31.572425",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.532985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sonar_csv_path = \"../input/mines-vs-rocks/sonar.all-data.csv\"\n",
    "sonar_data = pd.read_csv(sonar_csv_path, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae33c0b",
   "metadata": {
    "papermill": {
     "duration": 0.010366,
     "end_time": "2023-04-30T19:56:31.593387",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.583021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now have a pandas DataFrame encapsulating the sonar data, and can proceed with our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31210d5",
   "metadata": {
    "papermill": {
     "duration": 0.010263,
     "end_time": "2023-04-30T19:56:31.614006",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.603743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Explore the Data\n",
    "\n",
    "I first will use the describe method from pandas to take a look at my data to see what format it is in, if it is normalized, and where the labels exist for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f288be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:31.636386Z",
     "iopub.status.busy": "2023-04-30T19:56:31.635959Z",
     "iopub.status.idle": "2023-04-30T19:56:31.812395Z",
     "shell.execute_reply": "2023-04-30T19:56:31.810914Z"
    },
    "papermill": {
     "duration": 0.191129,
     "end_time": "2023-04-30T19:56:31.815568",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.624439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbc5f5",
   "metadata": {
    "papermill": {
     "duration": 0.010175,
     "end_time": "2023-04-30T19:56:31.836461",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.826286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The first thing I notice about this data is that it is not normalized. The minimum and maximum values are different for each feature, which can lead to features with larger numbers being misconstrued as more important by the machine learning algorithm. It is important to normalize this later on.\n",
    "\n",
    "Using the .describe() method will only show columns with numeric data, and I know the labels will be an \"R\" or \"M\" from exploring the data on the Kaggle website. To see these labels and any other columns that might not be numeric, I can select all types except float. I could also use the method sonar_data.head() to look at everything, but this omits many columns in the middle of the dataset because of its size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3bf3ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:31.859821Z",
     "iopub.status.busy": "2023-04-30T19:56:31.859369Z",
     "iopub.status.idle": "2023-04-30T19:56:31.876124Z",
     "shell.execute_reply": "2023-04-30T19:56:31.874564Z"
    },
    "papermill": {
     "duration": 0.031842,
     "end_time": "2023-04-30T19:56:31.878866",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.847024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    60\n",
       "0    R\n",
       "1    R\n",
       "2    R\n",
       "3    R\n",
       "4    R\n",
       "..  ..\n",
       "203  M\n",
       "204  M\n",
       "205  M\n",
       "206  M\n",
       "207  M\n",
       "\n",
       "[208 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.select_dtypes(exclude=['float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23257f",
   "metadata": {
    "papermill": {
     "duration": 0.010383,
     "end_time": "2023-04-30T19:56:31.900142",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.889759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I know that the labels are in column 60, which I can use later on when I am splitting the data into features and labels. This also tells me everything else in the dataframe are columns of floats that represent the features.\n",
    "\n",
    "Let's take a look at the distribution of values in our label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1897a214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:31.925239Z",
     "iopub.status.busy": "2023-04-30T19:56:31.924133Z",
     "iopub.status.idle": "2023-04-30T19:56:31.934430Z",
     "shell.execute_reply": "2023-04-30T19:56:31.932796Z"
    },
    "papermill": {
     "duration": 0.026075,
     "end_time": "2023-04-30T19:56:31.937116",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.911041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M    111\n",
      "R     97\n",
      "Name: 60, dtype: int64\n",
      "Total objects: 208\n"
     ]
    }
   ],
   "source": [
    "label_distribution = sonar_data[60].value_counts()\n",
    "print(label_distribution)\n",
    "print(\"Total objects: \" + str(len(sonar_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1fa595",
   "metadata": {
    "papermill": {
     "duration": 0.010477,
     "end_time": "2023-04-30T19:56:31.958659",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.948182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 111 data objects labeled \"mine\", and 97 data objects labeled \"rock\". This is not evenly distributed, so balancing the label weights during training might be advantageous. This also means there are 208 total data objects, which we confirm with the len() function above.\n",
    "\n",
    "The next step is to make sure there are no null values I need to resolve. This is very simple with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4f3adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:31.982579Z",
     "iopub.status.busy": "2023-04-30T19:56:31.982148Z",
     "iopub.status.idle": "2023-04-30T19:56:31.992274Z",
     "shell.execute_reply": "2023-04-30T19:56:31.991119Z"
    },
    "papermill": {
     "duration": 0.025362,
     "end_time": "2023-04-30T19:56:31.994966",
     "exception": false,
     "start_time": "2023-04-30T19:56:31.969604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62399514",
   "metadata": {
    "papermill": {
     "duration": 0.010714,
     "end_time": "2023-04-30T19:56:32.016889",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.006175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is no \"NaN\" values that I need to fix. Next, I want to look at the head and make sure I thoroughly explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece26bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:32.041149Z",
     "iopub.status.busy": "2023-04-30T19:56:32.040649Z",
     "iopub.status.idle": "2023-04-30T19:56:32.070348Z",
     "shell.execute_reply": "2023-04-30T19:56:32.068938Z"
    },
    "papermill": {
     "duration": 0.044739,
     "end_time": "2023-04-30T19:56:32.072740",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.028001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ba735",
   "metadata": {
    "papermill": {
     "duration": 0.010674,
     "end_time": "2023-04-30T19:56:32.094761",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.084087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we can verify the dimensions are as expected. There are 61 dimensions, with the last column being the labels. this is what we expect. Next let's look at some example values. The .head() method does not show the values in the middle, so let's use a for loop to print out a sample row to make sure the values in the middle are float values like we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f608f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:32.118968Z",
     "iopub.status.busy": "2023-04-30T19:56:32.118504Z",
     "iopub.status.idle": "2023-04-30T19:56:32.126473Z",
     "shell.execute_reply": "2023-04-30T19:56:32.125272Z"
    },
    "papermill": {
     "duration": 0.023637,
     "end_time": "2023-04-30T19:56:32.129412",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.105775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2872\n",
      "0.4918\n",
      "0.6552\n",
      "0.6919\n",
      "0.7797\n",
      "0.7464\n",
      "0.9444\n",
      "1.0\n",
      "0.8874\n",
      "0.8024\n",
      "0.7818\n",
      "0.5212\n",
      "0.4052\n",
      "0.3957\n",
      "0.3914\n",
      "0.325\n",
      "0.32\n",
      "0.3271\n",
      "0.2767\n",
      "0.4423\n",
      "0.2028\n",
      "0.3788\n",
      "0.2947\n",
      "0.1984\n",
      "0.2341\n",
      "0.1306\n",
      "0.4182\n",
      "0.3835\n",
      "0.1057\n",
      "0.184\n",
      "0.197\n",
      "0.1674\n",
      "0.0583\n",
      "0.1401\n",
      "0.1628\n",
      "0.0621\n",
      "0.0203\n",
      "0.053\n",
      "0.0742\n",
      "0.0409\n",
      "0.0061\n",
      "0.0125\n"
     ]
    }
   ],
   "source": [
    "for i in range(9,51):\n",
    "    print(sonar_data.iloc[1,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3422cb",
   "metadata": {
    "papermill": {
     "duration": 0.010648,
     "end_time": "2023-04-30T19:56:32.151342",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.140694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "These values are float values, and in the range we expect. We are good to move on now to preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19d73a",
   "metadata": {
    "papermill": {
     "duration": 0.010804,
     "end_time": "2023-04-30T19:56:32.173864",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.163060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Preprocess the Data\n",
    "\n",
    "The most important thing to do before training and testing the data is normalization. As described above, this helps the machine learning algorithim not to overemphasize one feature over the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc688bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:32.198635Z",
     "iopub.status.busy": "2023-04-30T19:56:32.198194Z",
     "iopub.status.idle": "2023-04-30T19:56:35.060099Z",
     "shell.execute_reply": "2023-04-30T19:56:35.058884Z"
    },
    "papermill": {
     "duration": 2.877277,
     "end_time": "2023-04-30T19:56:35.062644",
     "exception": false,
     "start_time": "2023-04-30T19:56:32.185367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.204011</td>\n",
       "      <td>0.162180</td>\n",
       "      <td>0.139068</td>\n",
       "      <td>0.114342</td>\n",
       "      <td>0.173732</td>\n",
       "      <td>0.253615</td>\n",
       "      <td>0.320472</td>\n",
       "      <td>0.285114</td>\n",
       "      <td>0.252485</td>\n",
       "      <td>0.281652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160047</td>\n",
       "      <td>0.180031</td>\n",
       "      <td>0.265172</td>\n",
       "      <td>0.290669</td>\n",
       "      <td>0.197061</td>\n",
       "      <td>0.200555</td>\n",
       "      <td>0.213642</td>\n",
       "      <td>0.175035</td>\n",
       "      <td>0.216015</td>\n",
       "      <td>0.136425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.169550</td>\n",
       "      <td>0.141277</td>\n",
       "      <td>0.126242</td>\n",
       "      <td>0.110623</td>\n",
       "      <td>0.140888</td>\n",
       "      <td>0.158843</td>\n",
       "      <td>0.167175</td>\n",
       "      <td>0.187767</td>\n",
       "      <td>0.175311</td>\n",
       "      <td>0.192215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119607</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>0.183385</td>\n",
       "      <td>0.213474</td>\n",
       "      <td>0.160717</td>\n",
       "      <td>0.147080</td>\n",
       "      <td>0.164361</td>\n",
       "      <td>0.148051</td>\n",
       "      <td>0.170286</td>\n",
       "      <td>0.116190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.067938</td>\n",
       "      <td>0.057326</td>\n",
       "      <td>0.044163</td>\n",
       "      <td>0.079508</td>\n",
       "      <td>0.152714</td>\n",
       "      <td>0.209957</td>\n",
       "      <td>0.165215</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.142964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083914</td>\n",
       "      <td>0.092368</td>\n",
       "      <td>0.118831</td>\n",
       "      <td>0.127924</td>\n",
       "      <td>0.080499</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.096591</td>\n",
       "      <td>0.075515</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.057737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.157080</td>\n",
       "      <td>0.129447</td>\n",
       "      <td>0.107753</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.141517</td>\n",
       "      <td>0.220236</td>\n",
       "      <td>0.280438</td>\n",
       "      <td>0.235061</td>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138446</td>\n",
       "      <td>0.151213</td>\n",
       "      <td>0.235065</td>\n",
       "      <td>0.242690</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.165385</td>\n",
       "      <td>0.160511</td>\n",
       "      <td>0.125858</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.108545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.251106</td>\n",
       "      <td>0.202958</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.139563</td>\n",
       "      <td>0.237319</td>\n",
       "      <td>0.333042</td>\n",
       "      <td>0.407738</td>\n",
       "      <td>0.361852</td>\n",
       "      <td>0.334555</td>\n",
       "      <td>0.368082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207420</td>\n",
       "      <td>0.227175</td>\n",
       "      <td>0.374026</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.260897</td>\n",
       "      <td>0.287642</td>\n",
       "      <td>0.229977</td>\n",
       "      <td>0.281680</td>\n",
       "      <td>0.183025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.204011    0.162180    0.139068    0.114342    0.173732    0.253615   \n",
       "std      0.169550    0.141277    0.126242    0.110623    0.140888    0.158843   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.087389    0.067938    0.057326    0.044163    0.079508    0.152714   \n",
       "50%      0.157080    0.129447    0.107753    0.090942    0.141517    0.220236   \n",
       "75%      0.251106    0.202958    0.185447    0.139563    0.237319    0.333042   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9   ...          50  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.320472    0.285114    0.252485    0.281652  ...    0.160047   \n",
       "std      0.167175    0.187767    0.175311    0.192215  ...    0.119607   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.209957    0.165215    0.132571    0.142964  ...    0.083914   \n",
       "50%      0.280438    0.235061    0.214349    0.244673  ...    0.138446   \n",
       "75%      0.407738    0.361852    0.334555    0.368082  ...    0.207420   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               51          52          53          54          55          56  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.180031    0.265172    0.290669    0.197061    0.200555    0.213642   \n",
       "std      0.137432    0.183385    0.213474    0.160717    0.147080    0.164361   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.092368    0.118831    0.127924    0.080499    0.102564    0.096591   \n",
       "50%      0.151213    0.235065    0.242690    0.156463    0.165385    0.160511   \n",
       "75%      0.227175    0.374026    0.394737    0.260771    0.260897    0.287642   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               57          58          59  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.175035    0.216015    0.136425  \n",
       "std      0.148051    0.170286    0.116190  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.075515    0.098485    0.057737  \n",
       "50%      0.125858    0.173554    0.108545  \n",
       "75%      0.229977    0.281680    0.183025  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through each column, except the last one with the labels\n",
    "for i in range(len(sonar_data.columns)-1):\n",
    "    # Store the current column and its min and max value\n",
    "    current_col = sonar_data.iloc[:,i]\n",
    "    col_min = current_col.min()\n",
    "    col_max = current_col.max()\n",
    "    # Normalize each data point between 0 and 1 using the minimum and maximum value\n",
    "    for x in range(len(current_col)):\n",
    "        sonar_data.iloc[x,i] = (sonar_data.iloc[x,i] - col_min) / (col_max -  col_min)\n",
    "# Check to make sure the operation performed as expected\n",
    "sonar_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770a650",
   "metadata": {
    "papermill": {
     "duration": 0.010714,
     "end_time": "2023-04-30T19:56:35.084720",
     "exception": false,
     "start_time": "2023-04-30T19:56:35.074006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now you can see that the min and max values for each feature is the same: from 0 to 1. This means the code worked and the features are ready for training. \n",
    "\n",
    "For the perceptron class from sklearn, we do not need to transform the class label values to -1 and 1. The labels we have will be mapped to -1 and 1 for us, or whatever the values need to be for the desired activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf66d01",
   "metadata": {
    "papermill": {
     "duration": 0.010651,
     "end_time": "2023-04-30T19:56:35.106559",
     "exception": false,
     "start_time": "2023-04-30T19:56:35.095908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Prepare the Training and Test Data Sets\n",
    "\n",
    "With help from sklearn and their `train_test_split`, it is easy to split the data into training and testing sets. This function takes the array of features, and then the column of matching labels. With some slicing it is easy to split the original dataframe because the labels are in the last column. The function will split the data into four variables, described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01831479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:35.131406Z",
     "iopub.status.busy": "2023-04-30T19:56:35.130499Z",
     "iopub.status.idle": "2023-04-30T19:56:36.142991Z",
     "shell.execute_reply": "2023-04-30T19:56:36.141698Z"
    },
    "papermill": {
     "duration": 1.028343,
     "end_time": "2023-04-30T19:56:36.145956",
     "exception": false,
     "start_time": "2023-04-30T19:56:35.117613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sonar_data.iloc[:,:60], sonar_data.iloc[:,60], test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c295be",
   "metadata": {
    "papermill": {
     "duration": 0.010709,
     "end_time": "2023-04-30T19:56:36.167933",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.157224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, the data is split into features (labeled \"X\") for training and testing, and labels (labeled \"y\") for training and testing. I chose to hold back 20% of the data for testing, because that is a common value to use for the hold-out method. I also set the random state to 0, so my results are reproducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e422bc",
   "metadata": {
    "papermill": {
     "duration": 0.01069,
     "end_time": "2023-04-30T19:56:36.189695",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.179005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Instantiate and Configure a Perceptron\n",
    "\n",
    "After importing [sklearn.linear_model.Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html), I set a variable equal to the model. \n",
    "\n",
    "There are many parameters I can set for the model. I chose some basic ones to help improve my testing results. I set the penalty (regularization) parameter to \"l2\", which will add a penalty term to the model's loss function. This encourages the model to have smaller weights and help the model be more simple so it is not overfit. This is very important for this data set because there are 60 features and only 208 data objects (166 of which will be used for training). When a data set has a high number of features and a low number of data objects to be trained on, there will be a tendancy for overfitting. This is why I chose to use the \"l2\" regularization.\n",
    "\n",
    "The tolerance is when the model stops because the current loss is greater than the previous loss - the tolerance value. This stops the model when it is approaching an ideal set of weights and is not correcting itself as much. I kept this as the default value, 0.001. \n",
    "\n",
    "I again set a random_state to get reproducible results, and set the class_weight to 'balanced' so that the algoritihm would weigh the the class that is less frequent more heavily. This helps to not create a model that is biased toward a certain class because while training that class is seen more frequently. We saw earlier in the data exploration that there is a slight bias toward the \"mine\" class. That is why I chose to add this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77177734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:36.214626Z",
     "iopub.status.busy": "2023-04-30T19:56:36.213257Z",
     "iopub.status.idle": "2023-04-30T19:56:36.308295Z",
     "shell.execute_reply": "2023-04-30T19:56:36.306980Z"
    },
    "papermill": {
     "duration": 0.110717,
     "end_time": "2023-04-30T19:56:36.311449",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.200732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron_model = Perceptron(penalty='l2',tol=.001,random_state=0, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0c896",
   "metadata": {
    "papermill": {
     "duration": 0.010821,
     "end_time": "2023-04-30T19:56:36.333746",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.322925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model is no instantiated and ready to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4a945",
   "metadata": {
    "papermill": {
     "duration": 0.010709,
     "end_time": "2023-04-30T19:56:36.355757",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.345048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Train the Perceptron\n",
    "\n",
    "The next step is to train the model, which is easy with sklearn. All I need to do is use the fit method and supply the training features and labels. I print out the training accuracy achieved through all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26857109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:36.380627Z",
     "iopub.status.busy": "2023-04-30T19:56:36.380163Z",
     "iopub.status.idle": "2023-04-30T19:56:36.399739Z",
     "shell.execute_reply": "2023-04-30T19:56:36.397959Z"
    },
    "papermill": {
     "duration": 0.037054,
     "end_time": "2023-04-30T19:56:36.403923",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.366869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493975903614458\n"
     ]
    }
   ],
   "source": [
    "perceptron_model.fit(X_train,y_train)\n",
    "print(perceptron_model.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe165c43",
   "metadata": {
    "papermill": {
     "duration": 0.028784,
     "end_time": "2023-04-30T19:56:36.461531",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.432747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We achieved a training accuracy of 84.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daefc4",
   "metadata": {
    "papermill": {
     "duration": 0.024798,
     "end_time": "2023-04-30T19:56:36.517964",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.493166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 7: Test the Perceptron\n",
    "\n",
    "Now it's time to see how the model performs on unseen data. Here I print out the score of the model as it predicts on the withheld testing data and compares against the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6430ff62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:36.542247Z",
     "iopub.status.busy": "2023-04-30T19:56:36.541803Z",
     "iopub.status.idle": "2023-04-30T19:56:36.551287Z",
     "shell.execute_reply": "2023-04-30T19:56:36.549745Z"
    },
    "papermill": {
     "duration": 0.02505,
     "end_time": "2023-04-30T19:56:36.554329",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.529279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(perceptron_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e31ce2",
   "metadata": {
    "papermill": {
     "duration": 0.010884,
     "end_time": "2023-04-30T19:56:36.576459",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.565575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The accuracy I got was 85.7%. This is not as high as I would like it to be, but it is an improvement over the 67% or so accuracy I was initially getting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd359a",
   "metadata": {
    "papermill": {
     "duration": 0.01068,
     "end_time": "2023-04-30T19:56:36.598280",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.587600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 8: Demonstrate Making Predictions\n",
    "\n",
    "It is time to show what the trained model can do. I created a random array of data that has 60 values, all between 0 and 1, just like the true features. By plugging these features into the model and using the predict method, it will print a prediction. Here it predicts \"M\", which means a mine. I also commented out a line that takes the 5th row of the original dataframe and plugs that back into the model. This was just a test to see a different result. If you uncomment it and run the cell, the model will predict an \"R\", which means rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577503ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T19:56:36.623075Z",
     "iopub.status.busy": "2023-04-30T19:56:36.622038Z",
     "iopub.status.idle": "2023-04-30T19:56:36.631121Z",
     "shell.execute_reply": "2023-04-30T19:56:36.629671Z"
    },
    "papermill": {
     "duration": 0.024141,
     "end_time": "2023-04-30T19:56:36.633592",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.609451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M']\n"
     ]
    }
   ],
   "source": [
    "# Generate random data to test the predict method of the model\n",
    "random_features = pd.DataFrame([random.uniform(0, 1) for i in range(60)]).values.reshape(1,-1)\n",
    "#random_features = sonar_data.iloc[5,:60].values.reshape(1,-1)\n",
    "prediction = perceptron_model.predict(random_features)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad6a26",
   "metadata": {
    "papermill": {
     "duration": 0.011247,
     "end_time": "2023-04-30T19:56:36.656324",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.645077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can see the results of the prediction above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385099",
   "metadata": {
    "papermill": {
     "duration": 0.011086,
     "end_time": "2023-04-30T19:56:36.678675",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.667589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 9: Evaluate (and Improve?)\n",
    "\n",
    "I believe at first my model was overfitting the training data. I had an accuracy of about 67%, and then after I applied regularization and balanced class weights to the model the accuracy increase to almost 86%. This model is currently underperforming, and is not a good classifier. It performed worse than trained humans, who had accuracies from 88% to 97%. I still believe there is some overfitting going on. I think the next step is to reduce the size of the feature space. There are 60 features, which can lead to the perceptron trying to fit the noise instead of the pattern. A dimension reduction technique like PCA could be useful. I also think early stopping might be a good option to use to stop the model before the overfitting happens. That is a parameter I could set easily in the sklearn class. The last thing I would do is cross-validation with the data, so the model could be trained and tested on each fold. This would allow optimal tuning of the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1ec9e",
   "metadata": {
    "papermill": {
     "duration": 0.010892,
     "end_time": "2023-04-30T19:56:36.700989",
     "exception": false,
     "start_time": "2023-04-30T19:56:36.690097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In part 2 I explored the sonar dataset, used the sklearn perceptron to train and test a model to predict if the readings indicated a mine or solid rock, and tried to improve the model accuracy by tuning hyperparameters. It was especially important to normalize each feature, understand my model was overfitting the data by observing how the testing accuracy would change as I adjusted hyperparameters, and look back and study things I could do to further improve my results, as described above. I would like to have a graphical visualization of the accuracy through each epoch, especially if it also plotted the testing accuracy at each epoch step. That would make it very clear when the model was overfitting. I also wonder if the perceptron is the best classifier for this data, and how you could tell? Because there are so many features, you cannot just plot it to see if it can be linearly separable.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.153177,
   "end_time": "2023-04-30T19:56:37.434976",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-30T19:56:17.281799",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
